{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"source/images/image.png","path":"images/image.png","modified":0,"renderable":0},{"_id":"source/images/动手学day1_crossloss.png","path":"images/动手学day1_crossloss.png","modified":0,"renderable":0},{"_id":"source/images/隐藏层.png","path":"images/隐藏层.png","modified":0,"renderable":0},{"_id":"themes/oranges/source/css/color-scheme.css","path":"css/color-scheme.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/base.css","path":"css/base.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/comments.css","path":"css/comments.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/github-markdown.css","path":"css/github-markdown.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/highlight.css","path":"css/highlight.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/oranges/source/images/avatar.png","path":"images/avatar.png","modified":0,"renderable":1},{"_id":"themes/oranges/source/images/favicon.png","path":"images/favicon.png","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/activeNav.js","path":"js/activeNav.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/backtotop.js","path":"js/backtotop.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/catalog.js","path":"js/catalog.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/codeCopy.js","path":"js/codeCopy.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/colorscheme.js","path":"js/colorscheme.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/fancybox.js","path":"js/fancybox.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/shares.js","path":"js/shares.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/clipboard.min.js","path":"plugins/clipboard.min.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/gitalk.min.js","path":"plugins/gitalk.min.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/gitalk.css","path":"plugins/gitalk.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/jquery.fancybox.min.css","path":"plugins/jquery.fancybox.min.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/jquery.fancybox.min.js","path":"plugins/jquery.fancybox.min.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/jquery.min.js","path":"plugins/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/valine.min.js","path":"plugins/valine.min.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/waline.css","path":"plugins/waline.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/waline.js","path":"plugins/waline.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/waline.mjs","path":"plugins/waline.mjs","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/figcaption/mac-block.css","path":"css/figcaption/mac-block.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/mathjax/tex-chtml.js","path":"plugins/mathjax/tex-chtml.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff","path":"plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff","path":"plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff","path":"plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff","path":"plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff","modified":0,"renderable":1},{"_id":"source/images/ReLU.png","path":"images/ReLU.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/_posts/动手学AI-多层感知机.md","hash":"bc1b9e7f47d3675961b512491ea422c447536bcc","modified":1760342927259},{"_id":"source/_posts/动手学AI-线性回归.md","hash":"6a97f1a5860c6d352850568724afb449b87a96b3","modified":1760278603768},{"_id":"source/categories/index.md","hash":"8c2c2b0f80f5d7a6786ec63326b2785ab97b93d1","modified":1760278603769},{"_id":"source/about/index.md","hash":"2f65f2ff3756c5e62c17edf8fa88b59faca9abb5","modified":1760278603769},{"_id":"source/friends/index.md","hash":"c4be274cb72e53d95f443c6aa63b90525ab9a6dc","modified":1760278603769},{"_id":"source/images/image.png","hash":"d60f944046d834a7537a496b06407dced5502943","modified":1760278603769},{"_id":"source/images/动手学day1_crossloss.png","hash":"ccab9bff7c892b5144be6b1526bab40f622471b4","modified":1760278603769},{"_id":"source/tags/index.md","hash":"a1233318fcf90089a75b0d272eaa48a43685ae1f","modified":1760278603770},{"_id":"source/images/隐藏层.png","hash":"71b3b6eb0f697a47709367b3767fd0dc4ae1fa3e","modified":1760278603770},{"_id":"themes/oranges/source/css/_common/layout/header.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1760279121924},{"_id":"themes/oranges/.gitignore","hash":"235a46a06c2464f3e8142d7b78d853ec621038b3","modified":1760279121919},{"_id":"themes/oranges/LICENSE","hash":"dbd3fa0b99b0acb027e8671cef34a6491e839758","modified":1760279121919},{"_id":"themes/oranges/README.md","hash":"c5ca9783e7a0351de1909af1b59e1abe2cce4a31","modified":1760279121919},{"_id":"themes/oranges/_config.yml","hash":"3513aea76bbfd34c288d342416989b73f5fbc26d","modified":1760279121920},{"_id":"themes/oranges/README-zh.md","hash":"3e9d9e066d298790df1663ea8a9bc598eddb7bcd","modified":1760279121919},{"_id":"themes/oranges/languages/de.yml","hash":"ad90132d331b9c5684129f2f08ddeaff27e1a4da","modified":1760279121920},{"_id":"themes/oranges/languages/default.yml","hash":"c0493633b1d07ec130b24ac3fd27717ffcd30731","modified":1760279121920},{"_id":"themes/oranges/languages/es.yml","hash":"a75cef476c5838772690240d23b36f334bfe147b","modified":1760279121920},{"_id":"themes/oranges/languages/fr.yml","hash":"dfc3fc2513f6d8a50babdb2076d5bcaba52339f7","modified":1760279121920},{"_id":"themes/oranges/languages/ko.yml","hash":"382182478ffa99f9a3c8d025bf7de1bb1cb06c45","modified":1760279121920},{"_id":"themes/oranges/languages/ja.yml","hash":"ebd19d2b2329767530145bc0822d1f883783049d","modified":1760279121920},{"_id":"themes/oranges/languages/nl.yml","hash":"728a17f83f604f59b52cf3fb0ce1b2434e96bf4c","modified":1760279121921},{"_id":"themes/oranges/languages/no.yml","hash":"2c2038d86738da9de2c4b3668c5e5928240380ce","modified":1760279121921},{"_id":"themes/oranges/languages/pt.yml","hash":"6d924968447e2b4a31de9ac1329326db89ab5a27","modified":1760279121921},{"_id":"themes/oranges/languages/ru.yml","hash":"509cb6b67625d5b46a38fab387d946c7a336cad4","modified":1760279121921},{"_id":"themes/oranges/languages/zh-CN.yml","hash":"133c650074fbf18e4f90bfacb34487cdbfcc9191","modified":1760279121921},{"_id":"themes/oranges/languages/zh-TW.yml","hash":"28ef519d0eccca0aa6f7637204fcdd8286532e78","modified":1760279121921},{"_id":"themes/oranges/layout/archive.ejs","hash":"ac08ee5bdb60b625896d16540ab444be844cab2b","modified":1760279121922},{"_id":"themes/oranges/layout/category.ejs","hash":"b731a87eab749d49b699af990149dfc53374f4ea","modified":1760279121923},{"_id":"themes/oranges/layout/index.ejs","hash":"9e05d7e8ca4bf38a0c36b8c1db29465b7cc561c7","modified":1760279121923},{"_id":"themes/oranges/layout/layout.ejs","hash":"a4ca62256a067680d703c0b76d9b71cdd9773f2d","modified":1760279121923},{"_id":"themes/oranges/layout/post.ejs","hash":"2dabf35adc38c640209f33eede3b9c92a2a9c6d2","modified":1760279121923},{"_id":"themes/oranges/layout/tag.ejs","hash":"209dc1ab4bfdf8123fc18a0ceefcd96787b44596","modified":1760279121923},{"_id":"themes/oranges/layout/_partial/backtotop.ejs","hash":"6b19f389755cbad990905c9284a13c70971e72b0","modified":1760279121921},{"_id":"themes/oranges/layout/_partial/clipboard.ejs","hash":"7fd17533fb4c5dd7524a381934ffcd5c27b0b6aa","modified":1760279121921},{"_id":"themes/oranges/layout/_partial/catalog.ejs","hash":"fb871254561b2b6e41d1e0195fc96684c6edc527","modified":1760279121921},{"_id":"themes/oranges/layout/_partial/comments.ejs","hash":"3e80be3f094767c672d7f34dd8611843641d6d44","modified":1760279121922},{"_id":"themes/oranges/layout/_partial/colorscheme.ejs","hash":"993af90279088a2934a857b4c51c8487e845d3e7","modified":1760279121921},{"_id":"themes/oranges/layout/_partial/footer.ejs","hash":"7e491f26c9eec689133f688a341d0bd2d3314a09","modified":1760279121922},{"_id":"themes/oranges/layout/_partial/header.ejs","hash":"929efe6f82f96ab26a4a501922036ec5085846e5","modified":1760279121922},{"_id":"themes/oranges/layout/_partial/mathjax.ejs","hash":"456981c33c943454f25fd025b76ec082f1dc3368","modified":1760279121922},{"_id":"themes/oranges/layout/_partial/navigation.ejs","hash":"449e17212e5bd98575a4c157a61cf8c801ef0e7b","modified":1760279121922},{"_id":"themes/oranges/layout/_partial/search.ejs","hash":"a6df7b553ff03b5f292571168aefe9e6dbba656a","modified":1760279121922},{"_id":"themes/oranges/source/css/color-scheme.css","hash":"b4a413e41e29e25e6472a1df2e8df36601c20174","modified":1760279121924},{"_id":"themes/oranges/layout/_partial/shares.ejs","hash":"5d50422f3949695cd28a525ef23536d632c271f7","modified":1760279121922},{"_id":"themes/oranges/source/css/base.css","hash":"d3d22be5257d03a61f619cbd192c5c7afa69b0a9","modified":1760279121924},{"_id":"themes/oranges/source/css/comments.css","hash":"09d81f9c8dd8f5a716b8622f25bcb414aeb00281","modified":1760279121924},{"_id":"themes/oranges/source/css/github-markdown.css","hash":"805b32a92605b78cdc625c8e9339f92e13dfa575","modified":1760279121925},{"_id":"themes/oranges/source/css/highlight.css","hash":"960cfd4a2f66434c36d1d72731bb2e57a3740345","modified":1760279121925},{"_id":"themes/oranges/source/images/avatar.png","hash":"642ef58c0781d0f1885775ddd349ca7af65f24b0","modified":1760279121925},{"_id":"themes/oranges/source/css/main.styl","hash":"988a14ee1cdad166c7a9c9bd58d025d5594076c1","modified":1760279121925},{"_id":"themes/oranges/source/images/favicon.png","hash":"21cf9c2e9c36c244a6542a3b6c220f13fa1a67cc","modified":1760279121925},{"_id":"themes/oranges/source/js/activeNav.js","hash":"65150cda5900eab5dc4652f9698512bafb5833e3","modified":1760279121925},{"_id":"themes/oranges/source/js/backtotop.js","hash":"1193edfd3cb032a72ba1c100be83bd459a2f63ac","modified":1760279121926},{"_id":"themes/oranges/source/js/catalog.js","hash":"3c8215aaad1ef05323b74b5f135b0bad2a3385a6","modified":1760279121926},{"_id":"themes/oranges/source/js/codeCopy.js","hash":"fd4b24a2cd985d857f6ab51853e72b623ce8765a","modified":1760279121926},{"_id":"themes/oranges/source/js/colorscheme.js","hash":"8b5626c40874c6cd39ba0fdeb3f2f3439d85da7d","modified":1760279121926},{"_id":"themes/oranges/source/js/fancybox.js","hash":"ed4c22785ec2c0764792011be19258dce3995487","modified":1760279121926},{"_id":"themes/oranges/source/js/search.js","hash":"fbac709e9a6befddbf8b31fd571b072e3de27b73","modified":1760279121926},{"_id":"themes/oranges/source/js/shares.js","hash":"838daa82d612ce632762343dbf6153343d0036f8","modified":1760279121926},{"_id":"themes/oranges/source/plugins/clipboard.min.js","hash":"d62dcb0905e038e69ff24ab9eef9e3306d45535e","modified":1760279121926},{"_id":"themes/oranges/source/plugins/gitalk.css","hash":"4c0d5510ea487b0fe63e96464ab0b381565cc273","modified":1760279121927},{"_id":"themes/oranges/source/plugins/jquery.fancybox.min.css","hash":"2e6a66987dbc7a57bbfd2655bce166739b4ba426","modified":1760279121930},{"_id":"themes/oranges/source/plugins/waline.css","hash":"25364b45ca67bc573d18bd15e252db737cfa3c0d","modified":1760279121937},{"_id":"themes/oranges/source/css/figcaption/mac-block.css","hash":"db51c58260e441632734d9bc2d41be2aa31df7c1","modified":1760279121925},{"_id":"themes/oranges/source/css/_common/comments/index.styl","hash":"7085df46c9d80aa1ea1cd361d75dfb8815e2d3da","modified":1760279121923},{"_id":"themes/oranges/source/css/_common/comments/valine.styl","hash":"23ee39dc6bb081fcb424746d1cc0c9a8cfa43d81","modified":1760279121923},{"_id":"themes/oranges/source/css/_common/components/fancybox.styl","hash":"9cf143a4215d28f851e7dd47dfa613719f4756d0","modified":1760279121923},{"_id":"themes/oranges/source/css/_common/components/index.styl","hash":"6aa40f2cfe058c7b73dcde03586c71421725f842","modified":1760279121924},{"_id":"themes/oranges/source/css/_common/utils/index.styl","hash":"cf587cb1e8cb0cc6b47efa9944094e7558a15c92","modified":1760279121924},{"_id":"themes/oranges/source/css/_common/layout/footer.styl","hash":"c83b8b135e10327f6c9779b4708bb625adc6462c","modified":1760279121924},{"_id":"themes/oranges/source/css/_common/layout/index.styl","hash":"967a2fc81e5394c95b5172680d41c48c35942995","modified":1760279121924},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff","hash":"89d2c8d274693c5a6e250e96e2a2e26e25619079","modified":1760279121932},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff","hash":"9c98f9f022647eb802434947e062b569ccedd5f0","modified":1760279121932},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff","hash":"91a4025dd3b18ca6bda63a215869773705435041","modified":1760279121932},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff","hash":"6712cf0aa5c12edc1cbbca8a1732a9cde0854c48","modified":1760279121932},{"_id":"themes/oranges/source/plugins/jquery.fancybox.min.js","hash":"3154fd527a002788848d9fec61d522048890e516","modified":1760279121931},{"_id":"themes/oranges/source/plugins/jquery.min.js","hash":"0c3192b500a4fd550e483cf77a49806a5872185b","modified":1760279121931},{"_id":"themes/oranges/source/plugins/valine.min.js","hash":"c2f2b1b0346e28ceae19f4b3d174f033311aa060","modified":1760279121937},{"_id":"themes/oranges/source/plugins/waline.js","hash":"0f46a69ce82aefd07e445cb05ff6c941e83da9d1","modified":1760279121938},{"_id":"themes/oranges/source/plugins/waline.mjs","hash":"5ead58d3654d8618fc6929e862d03c787cbf21f7","modified":1760279121939},{"_id":"themes/oranges/source/plugins/gitalk.min.js","hash":"1df59d7e5481ac2917c7043b28883393675dcaf9","modified":1760279121930},{"_id":"themes/oranges/source/plugins/mathjax/tex-chtml.js","hash":"bef586271c8246d003509a68b8f11181d967847d","modified":1760279121936},{"_id":"public/search.xml","hash":"874802d2c0e4f41c5111ef3b17bd5666a83ddeed","modified":1760279142212},{"_id":"public/categories/index.html","hash":"1a5ccd109cedbcccf1dc491d2407e18d3436098f","modified":1760279142212},{"_id":"public/about/index.html","hash":"4fd9960507e6e5cc578b5646de429361bbf528d9","modified":1760279142212},{"_id":"public/friends/index.html","hash":"e00911851657d5f711057e37bc295ae6a9393651","modified":1760279142212},{"_id":"public/tags/index.html","hash":"19afdcc4514540d7a58d9f16ce8a2b9b55b7e792","modified":1760279142212},{"_id":"public/2025/10/01/动手学AI-多层感知机/index.html","hash":"700dd62c94d499ee99af5a525eb9d3b44a409aed","modified":1760279142212},{"_id":"public/2025/10/01/动手学AI-线性回归/index.html","hash":"22386ad5e30fa303ce213455abcb1140aeaf62d6","modified":1760279142212},{"_id":"public/archives/index.html","hash":"1400084503249581db65f3081d054583f216de86","modified":1760279142212},{"_id":"public/archives/2025/index.html","hash":"3bcd84f0c0083095a6ecb23dea132094654ce0bb","modified":1760279142212},{"_id":"public/archives/2025/10/index.html","hash":"79d3230a216739e00725161f0ef03a55084a066d","modified":1760279142212},{"_id":"public/categories/学习/index.html","hash":"aa9805e387d528df72bfdbabb38d797b229a658a","modified":1760279142212},{"_id":"public/index.html","hash":"a074db1bd41206991dcd179f6fcee33e810e899e","modified":1760279142212},{"_id":"public/tags/AI/index.html","hash":"c135e0fdc370e5f93640382701abed9cad8d9a47","modified":1760279142212},{"_id":"public/tags/机器学习/index.html","hash":"54c346a3d40229c98efa84601de845bfd0226a91","modified":1760279142212},{"_id":"public/images/image.png","hash":"d60f944046d834a7537a496b06407dced5502943","modified":1760279142212},{"_id":"public/images/动手学day1_crossloss.png","hash":"ccab9bff7c892b5144be6b1526bab40f622471b4","modified":1760279142212},{"_id":"public/images/avatar.png","hash":"642ef58c0781d0f1885775ddd349ca7af65f24b0","modified":1760279142212},{"_id":"public/images/favicon.png","hash":"21cf9c2e9c36c244a6542a3b6c220f13fa1a67cc","modified":1760279142212},{"_id":"public/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff","hash":"89d2c8d274693c5a6e250e96e2a2e26e25619079","modified":1760279142212},{"_id":"public/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff","hash":"9c98f9f022647eb802434947e062b569ccedd5f0","modified":1760279142212},{"_id":"public/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff","hash":"91a4025dd3b18ca6bda63a215869773705435041","modified":1760279142212},{"_id":"public/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff","hash":"6712cf0aa5c12edc1cbbca8a1732a9cde0854c48","modified":1760279142212},{"_id":"public/images/隐藏层.png","hash":"71b3b6eb0f697a47709367b3767fd0dc4ae1fa3e","modified":1760279142212},{"_id":"public/css/color-scheme.css","hash":"ec1e624f750e8caf5d463437b06bd1f705173467","modified":1760279142212},{"_id":"public/css/base.css","hash":"6fc8a265777c40336678c9b5cc033f7399b55459","modified":1760279142212},{"_id":"public/css/comments.css","hash":"b482de7b2e284ea0750aba0c0f85aca3d42c0af8","modified":1760279142212},{"_id":"public/css/github-markdown.css","hash":"0dc8e66ad2121924445150eb59a5f6091662f6f7","modified":1760279142212},{"_id":"public/css/main.css","hash":"bc42d2e3c1c705a20b96fa37313d8025c52c091e","modified":1760279142212},{"_id":"public/css/highlight.css","hash":"7ebfcbb58e87dd0436ab9538641eb6577fa7a8f8","modified":1760279142212},{"_id":"public/js/activeNav.js","hash":"06aa9a2985f1d328f0b7fe69a28bee31f16ebb1a","modified":1760279142212},{"_id":"public/js/backtotop.js","hash":"0be1bd072a7a34ce50d72376cad722023e772e6d","modified":1760279142212},{"_id":"public/js/catalog.js","hash":"3f895778af2029bff0cd588eeca0a8b64845065d","modified":1760279142212},{"_id":"public/js/codeCopy.js","hash":"3fab7bf3e0d22326440af1963e83448f8b8b4ab6","modified":1760279142212},{"_id":"public/js/colorscheme.js","hash":"1290f902b5651bf4d66187b5695ec90dc3ec70a8","modified":1760279142212},{"_id":"public/js/fancybox.js","hash":"b217d56f8db94498d7e272d164abac6ab1c07ddd","modified":1760279142212},{"_id":"public/js/search.js","hash":"374efc788268330edd7ca0c91a43e75f7ec4149c","modified":1760279142212},{"_id":"public/js/shares.js","hash":"aa0a3dd5c24efe7945351f7ac22d0f84a93e350c","modified":1760279142212},{"_id":"public/plugins/clipboard.min.js","hash":"9a7cb405f9beed005891587d41f76a0720893ffc","modified":1760279142212},{"_id":"public/plugins/gitalk.css","hash":"61d71cb30f5f34cbb1f2b5bc469784d6cb908c22","modified":1760279142212},{"_id":"public/plugins/jquery.fancybox.min.css","hash":"2e6a66987dbc7a57bbfd2655bce166739b4ba426","modified":1760279142212},{"_id":"public/plugins/jquery.fancybox.min.js","hash":"b2b093d8f5ffeee250c8d0d3a2285a213318e4ea","modified":1760279142212},{"_id":"public/plugins/waline.css","hash":"ba52d91d4685f8e07423a5651fea8a8f5151f457","modified":1760279142212},{"_id":"public/plugins/jquery.min.js","hash":"0dc32db4aa9c5f03f3b38c47d883dbd4fed13aae","modified":1760279142212},{"_id":"public/plugins/valine.min.js","hash":"d081a412c63411a75a3a880ddece65335d1c3ee8","modified":1760279142212},{"_id":"public/css/figcaption/mac-block.css","hash":"d923323312d78ecb40cb60c093dba36b0127db68","modified":1760279142212},{"_id":"public/plugins/waline.js","hash":"c26a8b22924813d883fb23e232063dce9b4e01c3","modified":1760279142212},{"_id":"public/plugins/waline.mjs","hash":"5ead58d3654d8618fc6929e862d03c787cbf21f7","modified":1760279142212},{"_id":"public/plugins/gitalk.min.js","hash":"564fc7c731d05fa70d71ef853a2c8cc7725739e2","modified":1760279142212},{"_id":"public/plugins/mathjax/tex-chtml.js","hash":"bef586271c8246d003509a68b8f11181d967847d","modified":1760279142212},{"_id":"source/images/ReLU.png","hash":"91a39ae0d58e8a294ed7532faa64bb5460405b4f","modified":1760342487675}],"Category":[{"name":"学习","_id":"cmgnsr37o0006f4gf7h935gwx"}],"Data":[],"Page":[{"title":"分类","date":"2022-02-23T09:56:00.000Z","aside":false,"top_img":false,"type":"categories","_content":"\ntest","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2022-02-23 17:56:00\naside: false\ntop_img: false\ntype: \"categories\"\n---\n\ntest","updated":"2025-10-12T14:16:43.769Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cmgnsr37i0000f4gfcss7d9e1","content":"<p>test</p>\n","excerpt":"","more":"<p>test</p>\n"},{"title":"tags","date":"2025-09-29T16:00:00.000Z","type":"about","categories":null,"tags":null,"_content":"\n# 👋 Hi, 我是王亿宽\n\n欢迎来到我的小小数字角落！🎉  \n我目前就读于 **武汉大学 国家网络安全学院**，专业是 **网络空间安全**。  \n学业上我保持着不错的成绩（专业排名前 20%，均分 91+ ✨），同时也热衷于科研、竞赛和开源分享。\n\n---\n\n## 🔬 研究兴趣\n- AI for Security \n- 安全 Agent 👀  \n- 深度学习与大模型安全 🤖  \n\n\n\n---\n\n## 🏆 竞赛与荣誉\n- **中国机器人及人工智能大赛** 🤖 全国一等奖 & 三等奖  \n- **全国大学生数学竞赛**（非数学A类）🏅 一等奖  \n- **全国大学生物联网设计竞赛** 📡 省部级一等奖  \n- **蓝桥杯大赛软件赛** 💻 省部级一等奖  \n- **国家励志奖学金、雷军研学基金、三好学生** 等校级荣誉  \n\n> 竞赛不仅让我积累了技术经验，也让我学会了如何快速组队、解决问题、打怪升级！⚔️\n\n---\n\n## 💻 技能树\n- **编程语言**：Python / C++ / Go / Rust / JavaScript / Node.js / LaTeX 等  \n- **框架工具**：PyTorch / Transformers / Pandas / React / Next.js / Django / Axum  \n- **开发方向**：算法实现、Web 全栈开发、科研原型系统、机器学习工程化  \n\n> 擅长把复杂问题拆解成清晰的架构模块，然后用代码一步步实现 💡。\n\n---\n\n## 🌟 校园生活\n- 担任过 **院学生会主席、班长**，也做过算法课程助教 🧑‍🏫。  \n- 既能统筹大型活动，也能给同学们讲清楚一道算法题。  \n- 我相信技术和组织力是相辅相成的：写好代码，也要把人和事安排得井井有条 ✨。\n\n---\n\n## 🎯 未来方向\n我希望继续在 **AI + Security** 方向深耕，把隐私保护和智能技术结合起来。  \n无论是科研论文还是工程实践，我都想探索一些真正能解决现实问题的方案 🚀。\n\n---\n\n<!-- 📬 **联系方式**  \n- Email: 2317705863@qq.com  \n- GitHub: [OneWide](https://github.com/OneWide)   -->\n\n---\n","source":"about/index.md","raw":"---\ntitle: tags\ndate: 2025-09-30\ntype: \"about\"\ncategories:\ntags:\n---\n\n# 👋 Hi, 我是王亿宽\n\n欢迎来到我的小小数字角落！🎉  \n我目前就读于 **武汉大学 国家网络安全学院**，专业是 **网络空间安全**。  \n学业上我保持着不错的成绩（专业排名前 20%，均分 91+ ✨），同时也热衷于科研、竞赛和开源分享。\n\n---\n\n## 🔬 研究兴趣\n- AI for Security \n- 安全 Agent 👀  \n- 深度学习与大模型安全 🤖  \n\n\n\n---\n\n## 🏆 竞赛与荣誉\n- **中国机器人及人工智能大赛** 🤖 全国一等奖 & 三等奖  \n- **全国大学生数学竞赛**（非数学A类）🏅 一等奖  \n- **全国大学生物联网设计竞赛** 📡 省部级一等奖  \n- **蓝桥杯大赛软件赛** 💻 省部级一等奖  \n- **国家励志奖学金、雷军研学基金、三好学生** 等校级荣誉  \n\n> 竞赛不仅让我积累了技术经验，也让我学会了如何快速组队、解决问题、打怪升级！⚔️\n\n---\n\n## 💻 技能树\n- **编程语言**：Python / C++ / Go / Rust / JavaScript / Node.js / LaTeX 等  \n- **框架工具**：PyTorch / Transformers / Pandas / React / Next.js / Django / Axum  \n- **开发方向**：算法实现、Web 全栈开发、科研原型系统、机器学习工程化  \n\n> 擅长把复杂问题拆解成清晰的架构模块，然后用代码一步步实现 💡。\n\n---\n\n## 🌟 校园生活\n- 担任过 **院学生会主席、班长**，也做过算法课程助教 🧑‍🏫。  \n- 既能统筹大型活动，也能给同学们讲清楚一道算法题。  \n- 我相信技术和组织力是相辅相成的：写好代码，也要把人和事安排得井井有条 ✨。\n\n---\n\n## 🎯 未来方向\n我希望继续在 **AI + Security** 方向深耕，把隐私保护和智能技术结合起来。  \n无论是科研论文还是工程实践，我都想探索一些真正能解决现实问题的方案 🚀。\n\n---\n\n<!-- 📬 **联系方式**  \n- Email: 2317705863@qq.com  \n- GitHub: [OneWide](https://github.com/OneWide)   -->\n\n---\n","updated":"2025-10-12T14:16:43.769Z","path":"about/index.html","comments":1,"layout":"page","_id":"cmgnsr37m0002f4gffyxzck39","content":"<h1 id=\"👋-Hi-我是王亿宽\"><a href=\"#👋-Hi-我是王亿宽\" class=\"headerlink\" title=\"👋 Hi, 我是王亿宽\"></a>👋 Hi, 我是王亿宽</h1><p>欢迎来到我的小小数字角落！🎉<br>我目前就读于 <strong>武汉大学 国家网络安全学院</strong>，专业是 <strong>网络空间安全</strong>。<br>学业上我保持着不错的成绩（专业排名前 20%，均分 91+ ✨），同时也热衷于科研、竞赛和开源分享。</p>\n<hr>\n<h2 id=\"🔬-研究兴趣\"><a href=\"#🔬-研究兴趣\" class=\"headerlink\" title=\"🔬 研究兴趣\"></a>🔬 研究兴趣</h2><ul>\n<li>AI for Security </li>\n<li>安全 Agent 👀  </li>\n<li>深度学习与大模型安全 🤖</li>\n</ul>\n<hr>\n<h2 id=\"🏆-竞赛与荣誉\"><a href=\"#🏆-竞赛与荣誉\" class=\"headerlink\" title=\"🏆 竞赛与荣誉\"></a>🏆 竞赛与荣誉</h2><ul>\n<li><strong>中国机器人及人工智能大赛</strong> 🤖 全国一等奖 &amp; 三等奖  </li>\n<li><strong>全国大学生数学竞赛</strong>（非数学A类）🏅 一等奖  </li>\n<li><strong>全国大学生物联网设计竞赛</strong> 📡 省部级一等奖  </li>\n<li><strong>蓝桥杯大赛软件赛</strong> 💻 省部级一等奖  </li>\n<li><strong>国家励志奖学金、雷军研学基金、三好学生</strong> 等校级荣誉</li>\n</ul>\n<blockquote>\n<p>竞赛不仅让我积累了技术经验，也让我学会了如何快速组队、解决问题、打怪升级！⚔️</p>\n</blockquote>\n<hr>\n<h2 id=\"💻-技能树\"><a href=\"#💻-技能树\" class=\"headerlink\" title=\"💻 技能树\"></a>💻 技能树</h2><ul>\n<li><strong>编程语言</strong>：Python &#x2F; C++ &#x2F; Go &#x2F; Rust &#x2F; JavaScript &#x2F; Node.js &#x2F; LaTeX 等  </li>\n<li><strong>框架工具</strong>：PyTorch &#x2F; Transformers &#x2F; Pandas &#x2F; React &#x2F; Next.js &#x2F; Django &#x2F; Axum  </li>\n<li><strong>开发方向</strong>：算法实现、Web 全栈开发、科研原型系统、机器学习工程化</li>\n</ul>\n<blockquote>\n<p>擅长把复杂问题拆解成清晰的架构模块，然后用代码一步步实现 💡。</p>\n</blockquote>\n<hr>\n<h2 id=\"🌟-校园生活\"><a href=\"#🌟-校园生活\" class=\"headerlink\" title=\"🌟 校园生活\"></a>🌟 校园生活</h2><ul>\n<li>担任过 <strong>院学生会主席、班长</strong>，也做过算法课程助教 🧑‍🏫。  </li>\n<li>既能统筹大型活动，也能给同学们讲清楚一道算法题。  </li>\n<li>我相信技术和组织力是相辅相成的：写好代码，也要把人和事安排得井井有条 ✨。</li>\n</ul>\n<hr>\n<h2 id=\"🎯-未来方向\"><a href=\"#🎯-未来方向\" class=\"headerlink\" title=\"🎯 未来方向\"></a>🎯 未来方向</h2><p>我希望继续在 <strong>AI + Security</strong> 方向深耕，把隐私保护和智能技术结合起来。<br>无论是科研论文还是工程实践，我都想探索一些真正能解决现实问题的方案 🚀。</p>\n<hr>\n<!-- 📬 **联系方式**  \n- Email: 2317705863@qq.com  \n- GitHub: [OneWide](https://github.com/OneWide)   -->\n\n<hr>\n","excerpt":"","more":"<h1 id=\"👋-Hi-我是王亿宽\"><a href=\"#👋-Hi-我是王亿宽\" class=\"headerlink\" title=\"👋 Hi, 我是王亿宽\"></a>👋 Hi, 我是王亿宽</h1><p>欢迎来到我的小小数字角落！🎉<br>我目前就读于 <strong>武汉大学 国家网络安全学院</strong>，专业是 <strong>网络空间安全</strong>。<br>学业上我保持着不错的成绩（专业排名前 20%，均分 91+ ✨），同时也热衷于科研、竞赛和开源分享。</p>\n<hr>\n<h2 id=\"🔬-研究兴趣\"><a href=\"#🔬-研究兴趣\" class=\"headerlink\" title=\"🔬 研究兴趣\"></a>🔬 研究兴趣</h2><ul>\n<li>AI for Security </li>\n<li>安全 Agent 👀  </li>\n<li>深度学习与大模型安全 🤖</li>\n</ul>\n<hr>\n<h2 id=\"🏆-竞赛与荣誉\"><a href=\"#🏆-竞赛与荣誉\" class=\"headerlink\" title=\"🏆 竞赛与荣誉\"></a>🏆 竞赛与荣誉</h2><ul>\n<li><strong>中国机器人及人工智能大赛</strong> 🤖 全国一等奖 &amp; 三等奖  </li>\n<li><strong>全国大学生数学竞赛</strong>（非数学A类）🏅 一等奖  </li>\n<li><strong>全国大学生物联网设计竞赛</strong> 📡 省部级一等奖  </li>\n<li><strong>蓝桥杯大赛软件赛</strong> 💻 省部级一等奖  </li>\n<li><strong>国家励志奖学金、雷军研学基金、三好学生</strong> 等校级荣誉</li>\n</ul>\n<blockquote>\n<p>竞赛不仅让我积累了技术经验，也让我学会了如何快速组队、解决问题、打怪升级！⚔️</p>\n</blockquote>\n<hr>\n<h2 id=\"💻-技能树\"><a href=\"#💻-技能树\" class=\"headerlink\" title=\"💻 技能树\"></a>💻 技能树</h2><ul>\n<li><strong>编程语言</strong>：Python &#x2F; C++ &#x2F; Go &#x2F; Rust &#x2F; JavaScript &#x2F; Node.js &#x2F; LaTeX 等  </li>\n<li><strong>框架工具</strong>：PyTorch &#x2F; Transformers &#x2F; Pandas &#x2F; React &#x2F; Next.js &#x2F; Django &#x2F; Axum  </li>\n<li><strong>开发方向</strong>：算法实现、Web 全栈开发、科研原型系统、机器学习工程化</li>\n</ul>\n<blockquote>\n<p>擅长把复杂问题拆解成清晰的架构模块，然后用代码一步步实现 💡。</p>\n</blockquote>\n<hr>\n<h2 id=\"🌟-校园生活\"><a href=\"#🌟-校园生活\" class=\"headerlink\" title=\"🌟 校园生活\"></a>🌟 校园生活</h2><ul>\n<li>担任过 <strong>院学生会主席、班长</strong>，也做过算法课程助教 🧑‍🏫。  </li>\n<li>既能统筹大型活动，也能给同学们讲清楚一道算法题。  </li>\n<li>我相信技术和组织力是相辅相成的：写好代码，也要把人和事安排得井井有条 ✨。</li>\n</ul>\n<hr>\n<h2 id=\"🎯-未来方向\"><a href=\"#🎯-未来方向\" class=\"headerlink\" title=\"🎯 未来方向\"></a>🎯 未来方向</h2><p>我希望继续在 <strong>AI + Security</strong> 方向深耕，把隐私保护和智能技术结合起来。<br>无论是科研论文还是工程实践，我都想探索一些真正能解决现实问题的方案 🚀。</p>\n<hr>\n<!-- 📬 **联系方式**  \n- Email: 2317705863@qq.com  \n- GitHub: [OneWide](https://github.com/OneWide)   -->\n\n<hr>\n"},{"title":"tags","date":"2025-09-29T16:00:00.000Z","type":"friends","categories":null,"tags":null,"_content":"","source":"friends/index.md","raw":"---\ntitle: tags\ndate: 2025-9-30\ntype: \"friends\"\ncategories:\ntags:\n---","updated":"2025-10-12T14:16:43.769Z","path":"friends/index.html","comments":1,"layout":"page","_id":"cmgnsr37o0005f4gfeu280ovd","content":"","excerpt":"","more":""},{"title":"tags","date":"2025-09-29T16:00:00.000Z","type":"tags","categories":null,"tags":null,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2025-09-30 \ntype: \"tags\"\ncategories:\ntags:\n---","updated":"2025-10-12T14:16:43.770Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cmgnsr37o0007f4gf2igt2ix9","content":"","excerpt":"","more":""}],"Post":[{"layout":"posts","title":"动手学AI_多层感知机","date":"2025-10-01T12:00:42.000Z","_content":"\n# 多层感知机是什么\n## 隐藏层\n前面说的线性模型是基于线性假设进行的，但是单纯的线性在真实世界里往往是不合理的，例如通过像素作为特征判断猫和狗，但是图片翻转后猫和狗仍不变，但像素特征改变，因此我们需要加一个或多个**隐藏层**来克服线性模型的限制，这就是**多层感知机（MLP）**\n![alt text](../images/隐藏层.png) \n如图，该多层感知机有4个输入，3个输出，5个隐藏单元，中间是全连接层。（层数为2，是全连接的，因为输入层不涉及运算）\n## 线性到非线性\n看上面的单层隐藏层的感知机，隐藏层其实就是加了一个隐藏层权重 ${W^1}$ 和隐藏层偏置${b^1}$,输出到输出层时把隐藏层的结果乘上输出层权重和偏置${W^2,b^2}$。\n\n也就是说加了隐藏层和没加差不多，毕竟我们可以通过合并隐藏层让 ${W = W^1 * W^2}$，${b = b^1 * W^2 + b^2}$来表示，那我们的多层架构就没有了任何意义，所以我们需要再仿射变换之后对每个隐藏单元应用非线性的激活函数 ${\\sigma}$，激活函数的输出称为活性值。 加入了激活函数之后，多层感知机就不会退化成线性模型。\n\n${H^{(1)} = \\sigma_1(XW^1+b^1)}$ ，${H^{(2)}=\\sigma_2(H^{(1)}W^2 +b^2)}$\n\n## 激活函数\nReLU函数，以0为活性值，仅保留正元素并丢弃所有负元素。\n\n```py\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny = torch.relu(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))\n```\n\n![alt text](../images/ReLU.png)\n该函数输入为负则导数为0，否则导数为1，输入0时不可导但使用0作为其导数，他的求导表现要么让参数消失，要么让参数通过，减轻了神经网络的梯度消失问题。\n\nsigmoid函数则是将输入变换为区间（0，1）上的输出：\n$${sigmoid(x) = \\frac{1}{1+exp(-x)}}$$","source":"_posts/动手学AI-多层感知机.md","raw":"---\nlayout: posts\ntitle: 动手学AI_多层感知机\ndate: 2025-10-01 20:00:42\ntags: \n - [AI]\n - [机器学习]\ncategories: \n - [学习]\n---\n\n# 多层感知机是什么\n## 隐藏层\n前面说的线性模型是基于线性假设进行的，但是单纯的线性在真实世界里往往是不合理的，例如通过像素作为特征判断猫和狗，但是图片翻转后猫和狗仍不变，但像素特征改变，因此我们需要加一个或多个**隐藏层**来克服线性模型的限制，这就是**多层感知机（MLP）**\n![alt text](../images/隐藏层.png) \n如图，该多层感知机有4个输入，3个输出，5个隐藏单元，中间是全连接层。（层数为2，是全连接的，因为输入层不涉及运算）\n## 线性到非线性\n看上面的单层隐藏层的感知机，隐藏层其实就是加了一个隐藏层权重 ${W^1}$ 和隐藏层偏置${b^1}$,输出到输出层时把隐藏层的结果乘上输出层权重和偏置${W^2,b^2}$。\n\n也就是说加了隐藏层和没加差不多，毕竟我们可以通过合并隐藏层让 ${W = W^1 * W^2}$，${b = b^1 * W^2 + b^2}$来表示，那我们的多层架构就没有了任何意义，所以我们需要再仿射变换之后对每个隐藏单元应用非线性的激活函数 ${\\sigma}$，激活函数的输出称为活性值。 加入了激活函数之后，多层感知机就不会退化成线性模型。\n\n${H^{(1)} = \\sigma_1(XW^1+b^1)}$ ，${H^{(2)}=\\sigma_2(H^{(1)}W^2 +b^2)}$\n\n## 激活函数\nReLU函数，以0为活性值，仅保留正元素并丢弃所有负元素。\n\n```py\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny = torch.relu(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))\n```\n\n![alt text](../images/ReLU.png)\n该函数输入为负则导数为0，否则导数为1，输入0时不可导但使用0作为其导数，他的求导表现要么让参数消失，要么让参数通过，减轻了神经网络的梯度消失问题。\n\nsigmoid函数则是将输入变换为区间（0，1）上的输出：\n$${sigmoid(x) = \\frac{1}{1+exp(-x)}}$$","slug":"动手学AI-多层感知机","published":1,"updated":"2025-10-13T08:08:47.259Z","_id":"cmgnsr37j0001f4gfbeo357xb","comments":1,"photos":[],"content":"<h1 id=\"多层感知机是什么\"><a href=\"#多层感知机是什么\" class=\"headerlink\" title=\"多层感知机是什么\"></a>多层感知机是什么</h1><h2 id=\"隐藏层\"><a href=\"#隐藏层\" class=\"headerlink\" title=\"隐藏层\"></a>隐藏层</h2><p>前面说的线性模型是基于线性假设进行的，但是单纯的线性在真实世界里往往是不合理的，例如通过像素作为特征判断猫和狗，但是图片翻转后猫和狗仍不变，但像素特征改变，因此我们需要加一个或多个<strong>隐藏层</strong>来克服线性模型的限制，这就是<strong>多层感知机（MLP）</strong><br><img src=\"/../images/%E9%9A%90%E8%97%8F%E5%B1%82.png\" alt=\"alt text\"><br>如图，该多层感知机有4个输入，3个输出，5个隐藏单元，中间是全连接层。（层数为2，是全连接的，因为输入层不涉及运算）</p>\n<h2 id=\"线性到非线性\"><a href=\"#线性到非线性\" class=\"headerlink\" title=\"线性到非线性\"></a>线性到非线性</h2><p>看上面的单层隐藏层的感知机，隐藏层其实就是加了一个隐藏层权重 ${W^1}$ 和隐藏层偏置${b^1}$,输出到输出层时把隐藏层的结果乘上输出层权重和偏置${W^2,b^2}$。</p>\n<p>也就是说加了隐藏层和没加差不多，毕竟我们可以通过合并隐藏层让 ${W &#x3D; W^1 * W^2}$，${b &#x3D; b^1 * W^2 + b^2}$来表示，那我们的多层架构就没有了任何意义，所以我们需要再仿射变换之后对每个隐藏单元应用非线性的激活函数 ${\\sigma}$，激活函数的输出称为活性值。 加入了激活函数之后，多层感知机就不会退化成线性模型。</p>\n<p>${H^{(1)} &#x3D; \\sigma_1(XW^1+b^1)}$ ，${H^{(2)}&#x3D;\\sigma_2(H^{(1)}W^2 +b^2)}$</p>\n<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><p>ReLU函数，以0为活性值，仅保留正元素并丢弃所有负元素。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(-<span class=\"number\">8.0</span>, <span class=\"number\">8.0</span>, <span class=\"number\">0.1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = torch.relu(x)</span><br><span class=\"line\">d2l.plot(x.detach(), y.detach(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;relu(x)&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/../images/ReLU.png\" alt=\"alt text\"><br>该函数输入为负则导数为0，否则导数为1，输入0时不可导但使用0作为其导数，他的求导表现要么让参数消失，要么让参数通过，减轻了神经网络的梯度消失问题。</p>\n<p>sigmoid函数则是将输入变换为区间（0，1）上的输出：<br>$${sigmoid(x) &#x3D; \\frac{1}{1+exp(-x)}}$$</p>\n","excerpt":"","more":"<h1 id=\"多层感知机是什么\"><a href=\"#多层感知机是什么\" class=\"headerlink\" title=\"多层感知机是什么\"></a>多层感知机是什么</h1><h2 id=\"隐藏层\"><a href=\"#隐藏层\" class=\"headerlink\" title=\"隐藏层\"></a>隐藏层</h2><p>前面说的线性模型是基于线性假设进行的，但是单纯的线性在真实世界里往往是不合理的，例如通过像素作为特征判断猫和狗，但是图片翻转后猫和狗仍不变，但像素特征改变，因此我们需要加一个或多个<strong>隐藏层</strong>来克服线性模型的限制，这就是<strong>多层感知机（MLP）</strong><br><img src=\"/../images/%E9%9A%90%E8%97%8F%E5%B1%82.png\" alt=\"alt text\"><br>如图，该多层感知机有4个输入，3个输出，5个隐藏单元，中间是全连接层。（层数为2，是全连接的，因为输入层不涉及运算）</p>\n<h2 id=\"线性到非线性\"><a href=\"#线性到非线性\" class=\"headerlink\" title=\"线性到非线性\"></a>线性到非线性</h2><p>看上面的单层隐藏层的感知机，隐藏层其实就是加了一个隐藏层权重 ${W^1}$ 和隐藏层偏置${b^1}$,输出到输出层时把隐藏层的结果乘上输出层权重和偏置${W^2,b^2}$。</p>\n<p>也就是说加了隐藏层和没加差不多，毕竟我们可以通过合并隐藏层让 ${W &#x3D; W^1 * W^2}$，${b &#x3D; b^1 * W^2 + b^2}$来表示，那我们的多层架构就没有了任何意义，所以我们需要再仿射变换之后对每个隐藏单元应用非线性的激活函数 ${\\sigma}$，激活函数的输出称为活性值。 加入了激活函数之后，多层感知机就不会退化成线性模型。</p>\n<p>${H^{(1)} &#x3D; \\sigma_1(XW^1+b^1)}$ ，${H^{(2)}&#x3D;\\sigma_2(H^{(1)}W^2 +b^2)}$</p>\n<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><p>ReLU函数，以0为活性值，仅保留正元素并丢弃所有负元素。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(-<span class=\"number\">8.0</span>, <span class=\"number\">8.0</span>, <span class=\"number\">0.1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = torch.relu(x)</span><br><span class=\"line\">d2l.plot(x.detach(), y.detach(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;relu(x)&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/../images/ReLU.png\" alt=\"alt text\"><br>该函数输入为负则导数为0，否则导数为1，输入0时不可导但使用0作为其导数，他的求导表现要么让参数消失，要么让参数通过，减轻了神经网络的梯度消失问题。</p>\n<p>sigmoid函数则是将输入变换为区间（0，1）上的输出：<br>$${sigmoid(x) &#x3D; \\frac{1}{1+exp(-x)}}$$</p>\n"},{"title":"动手学AI_线性回归","date":"2025-10-01T02:36:35.000Z","_content":"# Softmax回归\n## 从零实现\n### 初始化\n这里使用的Fashion-MNIST数据集图像为28*28，文中视为一个展平的向量，把每个像素看作一个特征。\n\nSoftmax的输出与判定的类别是**一样多的**，因此构筑成784\\*10的矩阵，偏置${b}$应该是1\\*10的向量，把权重W用正态分布初始化，把偏置用0初始化。\n\n``` python\nnum_inputs = 784 #输出维度\nnum_outputs = 10 #输入维度\n\nW = torch.normal(0,0.01,size=(num_inputs,num_outputs),requires_grad = True)\nb = torch.zeros(num_outputs,requires_grad = True)\n```\n> normal用于使用正态分布标准化变量，规定向量维度，requires_grad表示需要记录梯度，便于反向传播求导\n\n### 定义Softmax\n\nSoftmax由三步组成：\n- 对每个项求幂\n- 对每一行求和，得到每个样本的规范化常数\n- 对每一行除以规范化常数，确保结果和为1\n  \n \n\n![alt text](../images/image.png)\n\n\n于是我们根据上述步骤规定：\n```python\ndef softmax(X):\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepim = True) #轴1是行，0是列\n    return X_exp / partition\n```\n\n### 定义模型 \n模型，对应的就是输入如何通过映射到输出，这里我们是把像素点当作特征，那么我们就用reshape将原始图像转为向量。\n\n```py\ndef net(X):\n    return softmax(torch.matul(X.reshape((-1,W.shape[0]),W)+b))\n```\n\n### 定义损失函数\n使用交叉熵损失函数来进行定义损失。我们先创建一个数据样本y_hat，包含2个样本在3个类别的预测概率，以及对应的标签y。我们规定标签y在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。然后使用y作为y_hat中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。\n\n```python\ny = torch.tensor([0, 2])\ny_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ny_hat[[0, 1], y]\n```\n>这里用到了高级索引，y_hat[[0, 1], y] 相当于“从第0行取第 y[0] 个元素，从第1行取第 y[1] 个元素”\n\n下面我们定义交叉熵损失：\n\n```py\ndef cross_entropy(y_hat, y):\n    return - torch.log(y_hat[range(len(y_hat)), y])\n```\n交叉熵损失为：\n![alt text](../images/动手学day1_crossloss.png)\n\n### 分类精度\n我们上面已经给出了y_hat的预测分类，下面要看分类的精度如何。\n\n```py\ndef accuracy(y_hat, y):  \n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: #如果是高纬，就取最大值作为下标判断类别\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y #比较是否相等\n    return float(cmp.type(y.dtype).sum()) #返回相等的个数\n```\n下面判断在整个数据集上评估模型的准确率：\n```py\ndef evaluate_accuracy(net,data_iter):\n    if isinstance(net,torch.nn.Moudle):\n        net.eval() #把模型设置为评估模式，关闭dropout，batchnorm等训练行为\n    metric = Accumlator(2) #正确预测数、预测总数\n    with torch.no_grad():\n        for X,y in data_iter:\n            metric.add(accuracy(net(X),y),y.numel())\n    return metric[0] / metric[1]\n```\n这里的Accumulator是\n```py\nclass Accumulator:  #@save\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n```\n用于对多个变量进行累加,我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。\n\n### 训练\n```py\ndef train_epoch_ch3(net, train_iter, loss, updater):  #@save\n    \"\"\"训练模型一个迭代周期（定义见第3章）\"\"\"\n    # 将模型设置为训练模式\n    if isinstance(net, torch.nn.Module):\n        net.train()\n    # 训练损失总和、训练准确度总和、样本数\n    metric = Accumulator(3)\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        if isinstance(updater, torch.optim.Optimizer):\n            # 使用PyTorch内置的优化器和损失函数\n            updater.zero_grad()\n            l.mean().backward()\n            updater.step()\n        else:\n            # 使用定制的优化器和损失函数\n            l.sum().backward()\n            updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练精度\n    return metric[0] / metric[2], metric[1] / metric[2]\n```\n\n## 简洁实现\n```py\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n# PyTorch不会隐式地调整输入的形状。因此，\n# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状\nnet = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\nloss = nn.CrossEntropyLoss(reduction='none')\ntrainer = torch.optim.SGD(net.parameters(), lr=0.1)\nnum_epochs = 10\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```","source":"_posts/动手学AI-线性回归.md","raw":"---\ntitle: 动手学AI_线性回归\ndate: 2025-10-01 10:36:35\ntags: \n - [AI]\n - [机器学习]\ncategories: \n - [学习]\n---\n# Softmax回归\n## 从零实现\n### 初始化\n这里使用的Fashion-MNIST数据集图像为28*28，文中视为一个展平的向量，把每个像素看作一个特征。\n\nSoftmax的输出与判定的类别是**一样多的**，因此构筑成784\\*10的矩阵，偏置${b}$应该是1\\*10的向量，把权重W用正态分布初始化，把偏置用0初始化。\n\n``` python\nnum_inputs = 784 #输出维度\nnum_outputs = 10 #输入维度\n\nW = torch.normal(0,0.01,size=(num_inputs,num_outputs),requires_grad = True)\nb = torch.zeros(num_outputs,requires_grad = True)\n```\n> normal用于使用正态分布标准化变量，规定向量维度，requires_grad表示需要记录梯度，便于反向传播求导\n\n### 定义Softmax\n\nSoftmax由三步组成：\n- 对每个项求幂\n- 对每一行求和，得到每个样本的规范化常数\n- 对每一行除以规范化常数，确保结果和为1\n  \n \n\n![alt text](../images/image.png)\n\n\n于是我们根据上述步骤规定：\n```python\ndef softmax(X):\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepim = True) #轴1是行，0是列\n    return X_exp / partition\n```\n\n### 定义模型 \n模型，对应的就是输入如何通过映射到输出，这里我们是把像素点当作特征，那么我们就用reshape将原始图像转为向量。\n\n```py\ndef net(X):\n    return softmax(torch.matul(X.reshape((-1,W.shape[0]),W)+b))\n```\n\n### 定义损失函数\n使用交叉熵损失函数来进行定义损失。我们先创建一个数据样本y_hat，包含2个样本在3个类别的预测概率，以及对应的标签y。我们规定标签y在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。然后使用y作为y_hat中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。\n\n```python\ny = torch.tensor([0, 2])\ny_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ny_hat[[0, 1], y]\n```\n>这里用到了高级索引，y_hat[[0, 1], y] 相当于“从第0行取第 y[0] 个元素，从第1行取第 y[1] 个元素”\n\n下面我们定义交叉熵损失：\n\n```py\ndef cross_entropy(y_hat, y):\n    return - torch.log(y_hat[range(len(y_hat)), y])\n```\n交叉熵损失为：\n![alt text](../images/动手学day1_crossloss.png)\n\n### 分类精度\n我们上面已经给出了y_hat的预测分类，下面要看分类的精度如何。\n\n```py\ndef accuracy(y_hat, y):  \n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: #如果是高纬，就取最大值作为下标判断类别\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y #比较是否相等\n    return float(cmp.type(y.dtype).sum()) #返回相等的个数\n```\n下面判断在整个数据集上评估模型的准确率：\n```py\ndef evaluate_accuracy(net,data_iter):\n    if isinstance(net,torch.nn.Moudle):\n        net.eval() #把模型设置为评估模式，关闭dropout，batchnorm等训练行为\n    metric = Accumlator(2) #正确预测数、预测总数\n    with torch.no_grad():\n        for X,y in data_iter:\n            metric.add(accuracy(net(X),y),y.numel())\n    return metric[0] / metric[1]\n```\n这里的Accumulator是\n```py\nclass Accumulator:  #@save\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n```\n用于对多个变量进行累加,我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。\n\n### 训练\n```py\ndef train_epoch_ch3(net, train_iter, loss, updater):  #@save\n    \"\"\"训练模型一个迭代周期（定义见第3章）\"\"\"\n    # 将模型设置为训练模式\n    if isinstance(net, torch.nn.Module):\n        net.train()\n    # 训练损失总和、训练准确度总和、样本数\n    metric = Accumulator(3)\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        if isinstance(updater, torch.optim.Optimizer):\n            # 使用PyTorch内置的优化器和损失函数\n            updater.zero_grad()\n            l.mean().backward()\n            updater.step()\n        else:\n            # 使用定制的优化器和损失函数\n            l.sum().backward()\n            updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练精度\n    return metric[0] / metric[2], metric[1] / metric[2]\n```\n\n## 简洁实现\n```py\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n# PyTorch不会隐式地调整输入的形状。因此，\n# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状\nnet = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\nloss = nn.CrossEntropyLoss(reduction='none')\ntrainer = torch.optim.SGD(net.parameters(), lr=0.1)\nnum_epochs = 10\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```","slug":"动手学AI-线性回归","published":1,"updated":"2025-10-12T14:16:43.768Z","comments":1,"layout":"post","photos":[],"_id":"cmgnsr37m0003f4gf0tij76ty","content":"<h1 id=\"Softmax回归\"><a href=\"#Softmax回归\" class=\"headerlink\" title=\"Softmax回归\"></a>Softmax回归</h1><h2 id=\"从零实现\"><a href=\"#从零实现\" class=\"headerlink\" title=\"从零实现\"></a>从零实现</h2><h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>这里使用的Fashion-MNIST数据集图像为28*28，文中视为一个展平的向量，把每个像素看作一个特征。</p>\n<p>Softmax的输出与判定的类别是<strong>一样多的</strong>，因此构筑成784*10的矩阵，偏置${b}$应该是1*10的向量，把权重W用正态分布初始化，把偏置用0初始化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs = <span class=\"number\">784</span> <span class=\"comment\">#输出维度</span></span><br><span class=\"line\">num_outputs = <span class=\"number\">10</span> <span class=\"comment\">#输入维度</span></span><br><span class=\"line\"></span><br><span class=\"line\">W = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">0.01</span>,size=(num_inputs,num_outputs),requires_grad = <span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(num_outputs,requires_grad = <span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>normal用于使用正态分布标准化变量，规定向量维度，requires_grad表示需要记录梯度，便于反向传播求导</p>\n</blockquote>\n<h3 id=\"定义Softmax\"><a href=\"#定义Softmax\" class=\"headerlink\" title=\"定义Softmax\"></a>定义Softmax</h3><p>Softmax由三步组成：</p>\n<ul>\n<li>对每个项求幂</li>\n<li>对每一行求和，得到每个样本的规范化常数</li>\n<li>对每一行除以规范化常数，确保结果和为1</li>\n</ul>\n<p><img src=\"/../images/image.png\" alt=\"alt text\"></p>\n<p>于是我们根据上述步骤规定：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    X_exp = torch.exp(X)</span><br><span class=\"line\">    partition = X_exp.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>, keepim = <span class=\"literal\">True</span>) <span class=\"comment\">#轴1是行，0是列</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_exp / partition</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"定义模型\"><a href=\"#定义模型\" class=\"headerlink\" title=\"定义模型\"></a>定义模型</h3><p>模型，对应的就是输入如何通过映射到输出，这里我们是把像素点当作特征，那么我们就用reshape将原始图像转为向量。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> softmax(torch.matul(X.reshape((-<span class=\"number\">1</span>,W.shape[<span class=\"number\">0</span>]),W)+b))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"定义损失函数\"><a href=\"#定义损失函数\" class=\"headerlink\" title=\"定义损失函数\"></a>定义损失函数</h3><p>使用交叉熵损失函数来进行定义损失。我们先创建一个数据样本y_hat，包含2个样本在3个类别的预测概率，以及对应的标签y。我们规定标签y在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。然后使用y作为y_hat中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.tensor([<span class=\"number\">0</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">y_hat = torch.tensor([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.6</span>], [<span class=\"number\">0.3</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>]])</span><br><span class=\"line\">y_hat[[<span class=\"number\">0</span>, <span class=\"number\">1</span>], y]</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>这里用到了高级索引，y_hat[[0, 1], y] 相当于“从第0行取第 y[0] 个元素，从第1行取第 y[1] 个元素”</p>\n</blockquote>\n<p>下面我们定义交叉熵损失：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cross_entropy</span>(<span class=\"params\">y_hat, y</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> - torch.log(y_hat[<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(y_hat)), y])</span><br></pre></td></tr></table></figure>\n<p>交叉熵损失为：<br><img src=\"/../images/%E5%8A%A8%E6%89%8B%E5%AD%A6day1_crossloss.png\" alt=\"alt text\"></p>\n<h3 id=\"分类精度\"><a href=\"#分类精度\" class=\"headerlink\" title=\"分类精度\"></a>分类精度</h3><p>我们上面已经给出了y_hat的预测分类，下面要看分类的精度如何。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_hat, y</span>):  </span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(y_hat.shape) &gt; <span class=\"number\">1</span> <span class=\"keyword\">and</span> y_hat.shape[<span class=\"number\">1</span>] &gt; <span class=\"number\">1</span>: <span class=\"comment\">#如果是高纬，就取最大值作为下标判断类别</span></span><br><span class=\"line\">        y_hat = y_hat.argmax(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    cmp = y_hat.<span class=\"built_in\">type</span>(y.dtype) == y <span class=\"comment\">#比较是否相等</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">float</span>(cmp.<span class=\"built_in\">type</span>(y.dtype).<span class=\"built_in\">sum</span>()) <span class=\"comment\">#返回相等的个数</span></span><br></pre></td></tr></table></figure>\n<p>下面判断在整个数据集上评估模型的准确率：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_accuracy</span>(<span class=\"params\">net,data_iter</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net,torch.nn.Moudle):</span><br><span class=\"line\">        net.<span class=\"built_in\">eval</span>() <span class=\"comment\">#把模型设置为评估模式，关闭dropout，batchnorm等训练行为</span></span><br><span class=\"line\">    metric = Accumlator(<span class=\"number\">2</span>) <span class=\"comment\">#正确预测数、预测总数</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            metric.add(accuracy(net(X),y),y.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>\n<p>这里的Accumulator是</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Accumulator</span>:  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, n</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [<span class=\"number\">0.0</span>] * n</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">add</span>(<span class=\"params\">self, *args</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [a + <span class=\"built_in\">float</span>(b) <span class=\"keyword\">for</span> a, b <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(<span class=\"variable language_\">self</span>.data, args)]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [<span class=\"number\">0.0</span>] * <span class=\"built_in\">len</span>(<span class=\"variable language_\">self</span>.data)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.data[idx]</span><br></pre></td></tr></table></figure>\n<p>用于对多个变量进行累加,我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch_ch3</span>(<span class=\"params\">net, train_iter, loss, updater</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 将模型设置为训练模式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, torch.nn.Module):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">    <span class=\"comment\"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class=\"line\">    metric = Accumulator(<span class=\"number\">3</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">        <span class=\"comment\"># 计算梯度并更新参数</span></span><br><span class=\"line\">        y_hat = net(X)</span><br><span class=\"line\">        l = loss(y_hat, y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class=\"line\">            <span class=\"comment\"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class=\"line\">            updater.zero_grad()</span><br><span class=\"line\">            l.mean().backward()</span><br><span class=\"line\">            updater.step()</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用定制的优化器和损失函数</span></span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            updater(X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">        metric.add(<span class=\"built_in\">float</span>(l.<span class=\"built_in\">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class=\"line\">    <span class=\"comment\"># 返回训练损失和训练精度</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>], metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">2</span>]</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"简洁实现\"><a href=\"#简洁实现\" class=\"headerlink\" title=\"简洁实现\"></a>简洁实现</h2><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\"><span class=\"comment\"># PyTorch不会隐式地调整输入的形状。因此，</span></span><br><span class=\"line\"><span class=\"comment\"># 我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights);</span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<h1 id=\"Softmax回归\"><a href=\"#Softmax回归\" class=\"headerlink\" title=\"Softmax回归\"></a>Softmax回归</h1><h2 id=\"从零实现\"><a href=\"#从零实现\" class=\"headerlink\" title=\"从零实现\"></a>从零实现</h2><h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>这里使用的Fashion-MNIST数据集图像为28*28，文中视为一个展平的向量，把每个像素看作一个特征。</p>\n<p>Softmax的输出与判定的类别是<strong>一样多的</strong>，因此构筑成784*10的矩阵，偏置${b}$应该是1*10的向量，把权重W用正态分布初始化，把偏置用0初始化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs = <span class=\"number\">784</span> <span class=\"comment\">#输出维度</span></span><br><span class=\"line\">num_outputs = <span class=\"number\">10</span> <span class=\"comment\">#输入维度</span></span><br><span class=\"line\"></span><br><span class=\"line\">W = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">0.01</span>,size=(num_inputs,num_outputs),requires_grad = <span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(num_outputs,requires_grad = <span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>normal用于使用正态分布标准化变量，规定向量维度，requires_grad表示需要记录梯度，便于反向传播求导</p>\n</blockquote>\n<h3 id=\"定义Softmax\"><a href=\"#定义Softmax\" class=\"headerlink\" title=\"定义Softmax\"></a>定义Softmax</h3><p>Softmax由三步组成：</p>\n<ul>\n<li>对每个项求幂</li>\n<li>对每一行求和，得到每个样本的规范化常数</li>\n<li>对每一行除以规范化常数，确保结果和为1</li>\n</ul>\n<p><img src=\"/../images/image.png\" alt=\"alt text\"></p>\n<p>于是我们根据上述步骤规定：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    X_exp = torch.exp(X)</span><br><span class=\"line\">    partition = X_exp.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>, keepim = <span class=\"literal\">True</span>) <span class=\"comment\">#轴1是行，0是列</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_exp / partition</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"定义模型\"><a href=\"#定义模型\" class=\"headerlink\" title=\"定义模型\"></a>定义模型</h3><p>模型，对应的就是输入如何通过映射到输出，这里我们是把像素点当作特征，那么我们就用reshape将原始图像转为向量。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> softmax(torch.matul(X.reshape((-<span class=\"number\">1</span>,W.shape[<span class=\"number\">0</span>]),W)+b))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"定义损失函数\"><a href=\"#定义损失函数\" class=\"headerlink\" title=\"定义损失函数\"></a>定义损失函数</h3><p>使用交叉熵损失函数来进行定义损失。我们先创建一个数据样本y_hat，包含2个样本在3个类别的预测概率，以及对应的标签y。我们规定标签y在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。然后使用y作为y_hat中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.tensor([<span class=\"number\">0</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">y_hat = torch.tensor([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.6</span>], [<span class=\"number\">0.3</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>]])</span><br><span class=\"line\">y_hat[[<span class=\"number\">0</span>, <span class=\"number\">1</span>], y]</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>这里用到了高级索引，y_hat[[0, 1], y] 相当于“从第0行取第 y[0] 个元素，从第1行取第 y[1] 个元素”</p>\n</blockquote>\n<p>下面我们定义交叉熵损失：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cross_entropy</span>(<span class=\"params\">y_hat, y</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> - torch.log(y_hat[<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(y_hat)), y])</span><br></pre></td></tr></table></figure>\n<p>交叉熵损失为：<br><img src=\"/../images/%E5%8A%A8%E6%89%8B%E5%AD%A6day1_crossloss.png\" alt=\"alt text\"></p>\n<h3 id=\"分类精度\"><a href=\"#分类精度\" class=\"headerlink\" title=\"分类精度\"></a>分类精度</h3><p>我们上面已经给出了y_hat的预测分类，下面要看分类的精度如何。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_hat, y</span>):  </span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(y_hat.shape) &gt; <span class=\"number\">1</span> <span class=\"keyword\">and</span> y_hat.shape[<span class=\"number\">1</span>] &gt; <span class=\"number\">1</span>: <span class=\"comment\">#如果是高纬，就取最大值作为下标判断类别</span></span><br><span class=\"line\">        y_hat = y_hat.argmax(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    cmp = y_hat.<span class=\"built_in\">type</span>(y.dtype) == y <span class=\"comment\">#比较是否相等</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">float</span>(cmp.<span class=\"built_in\">type</span>(y.dtype).<span class=\"built_in\">sum</span>()) <span class=\"comment\">#返回相等的个数</span></span><br></pre></td></tr></table></figure>\n<p>下面判断在整个数据集上评估模型的准确率：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_accuracy</span>(<span class=\"params\">net,data_iter</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net,torch.nn.Moudle):</span><br><span class=\"line\">        net.<span class=\"built_in\">eval</span>() <span class=\"comment\">#把模型设置为评估模式，关闭dropout，batchnorm等训练行为</span></span><br><span class=\"line\">    metric = Accumlator(<span class=\"number\">2</span>) <span class=\"comment\">#正确预测数、预测总数</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            metric.add(accuracy(net(X),y),y.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>\n<p>这里的Accumulator是</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Accumulator</span>:  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, n</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [<span class=\"number\">0.0</span>] * n</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">add</span>(<span class=\"params\">self, *args</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [a + <span class=\"built_in\">float</span>(b) <span class=\"keyword\">for</span> a, b <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(<span class=\"variable language_\">self</span>.data, args)]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [<span class=\"number\">0.0</span>] * <span class=\"built_in\">len</span>(<span class=\"variable language_\">self</span>.data)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.data[idx]</span><br></pre></td></tr></table></figure>\n<p>用于对多个变量进行累加,我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch_ch3</span>(<span class=\"params\">net, train_iter, loss, updater</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 将模型设置为训练模式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, torch.nn.Module):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">    <span class=\"comment\"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class=\"line\">    metric = Accumulator(<span class=\"number\">3</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">        <span class=\"comment\"># 计算梯度并更新参数</span></span><br><span class=\"line\">        y_hat = net(X)</span><br><span class=\"line\">        l = loss(y_hat, y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class=\"line\">            <span class=\"comment\"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class=\"line\">            updater.zero_grad()</span><br><span class=\"line\">            l.mean().backward()</span><br><span class=\"line\">            updater.step()</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用定制的优化器和损失函数</span></span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            updater(X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">        metric.add(<span class=\"built_in\">float</span>(l.<span class=\"built_in\">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class=\"line\">    <span class=\"comment\"># 返回训练损失和训练精度</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>], metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">2</span>]</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"简洁实现\"><a href=\"#简洁实现\" class=\"headerlink\" title=\"简洁实现\"></a>简洁实现</h2><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\"><span class=\"comment\"># PyTorch不会隐式地调整输入的形状。因此，</span></span><br><span class=\"line\"><span class=\"comment\"># 我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights);</span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>"}],"PostAsset":[],"PostCategory":[{"post_id":"cmgnsr37j0001f4gfbeo357xb","category_id":"cmgnsr37o0006f4gf7h935gwx","_id":"cmgnsr37q000bf4gf6d661kcb"},{"post_id":"cmgnsr37m0003f4gf0tij76ty","category_id":"cmgnsr37o0006f4gf7h935gwx","_id":"cmgnsr37r000ff4gfeac0dq6x"}],"PostTag":[{"post_id":"cmgnsr37j0001f4gfbeo357xb","tag_id":"cmgnsr37n0004f4gfdnux759b","_id":"cmgnsr37q000cf4gf1fe39noo"},{"post_id":"cmgnsr37j0001f4gfbeo357xb","tag_id":"cmgnsr37p0008f4gf83cta7v2","_id":"cmgnsr37q000df4gfhmeu6oza"},{"post_id":"cmgnsr37m0003f4gf0tij76ty","tag_id":"cmgnsr37n0004f4gfdnux759b","_id":"cmgnsr37r000gf4gfd9g54pq0"},{"post_id":"cmgnsr37m0003f4gf0tij76ty","tag_id":"cmgnsr37p0008f4gf83cta7v2","_id":"cmgnsr37r000hf4gf002q3zyt"}],"Tag":[{"name":"AI","_id":"cmgnsr37n0004f4gfdnux759b"},{"name":"机器学习","_id":"cmgnsr37p0008f4gf83cta7v2"}]}}