{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"source/images/image.png","path":"images/image.png","modified":0,"renderable":0},{"_id":"source/images/动手学day1_crossloss.png","path":"images/动手学day1_crossloss.png","modified":0,"renderable":0},{"_id":"source/images/隐藏层.png","path":"images/隐藏层.png","modified":0,"renderable":0},{"_id":"themes/oranges/source/css/color-scheme.css","path":"css/color-scheme.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/base.css","path":"css/base.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/comments.css","path":"css/comments.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/github-markdown.css","path":"css/github-markdown.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/highlight.css","path":"css/highlight.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/oranges/source/images/avatar.png","path":"images/avatar.png","modified":0,"renderable":1},{"_id":"themes/oranges/source/images/favicon.png","path":"images/favicon.png","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/activeNav.js","path":"js/activeNav.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/backtotop.js","path":"js/backtotop.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/catalog.js","path":"js/catalog.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/codeCopy.js","path":"js/codeCopy.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/colorscheme.js","path":"js/colorscheme.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/fancybox.js","path":"js/fancybox.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/js/shares.js","path":"js/shares.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/clipboard.min.js","path":"plugins/clipboard.min.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/gitalk.min.js","path":"plugins/gitalk.min.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/gitalk.css","path":"plugins/gitalk.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/jquery.fancybox.min.css","path":"plugins/jquery.fancybox.min.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/jquery.fancybox.min.js","path":"plugins/jquery.fancybox.min.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/jquery.min.js","path":"plugins/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/valine.min.js","path":"plugins/valine.min.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/waline.css","path":"plugins/waline.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/waline.js","path":"plugins/waline.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/waline.mjs","path":"plugins/waline.mjs","modified":0,"renderable":1},{"_id":"themes/oranges/source/css/figcaption/mac-block.css","path":"css/figcaption/mac-block.css","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/mathjax/tex-chtml.js","path":"plugins/mathjax/tex-chtml.js","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff","path":"plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff","path":"plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff","path":"plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff","modified":0,"renderable":1},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff","path":"plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff","modified":0,"renderable":1},{"_id":"source/images/ReLU.png","path":"images/ReLU.png","modified":0,"renderable":0},{"_id":"source/images/MetaFrame.png","path":"images/MetaFrame.png","modified":1,"renderable":0},{"_id":"source/images/human-llm-metacogiitive.png","path":"images/human-llm-metacogiitive.png","modified":1,"renderable":0},{"_id":"source/images/sigmoid_grad.png","path":"images/sigmoid_grad.png","modified":1,"renderable":0},{"_id":"source/images/metacognitive-prompt.png","path":"images/metacognitive-prompt.png","modified":1,"renderable":0},{"_id":"source/images/sigmoid.png","path":"images/sigmoid.png","modified":1,"renderable":0},{"_id":"source/images/genai.png","path":"images/genai.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/_posts/动手学AI-多层感知机.md","hash":"7ec3ab6c53f2ea1d2b920ab0eda334e5421735a6","modified":1761102819887},{"_id":"source/_posts/动手学AI-线性回归.md","hash":"a1a4614159e4eb1b282876b19f7287522bf0eae3","modified":1759320137922},{"_id":"source/categories/index.md","hash":"149a18a817618b106a00aa71baeeb9245794a6e9","modified":1759230780122},{"_id":"source/about/index.md","hash":"5a84e1e8208e84d077653b3edee65e82db15f00b","modified":1759241058051},{"_id":"source/friends/index.md","hash":"6d33f7fe995bd39750896a740a5555581d253fd9","modified":1759229744365},{"_id":"source/images/image.png","hash":"d60f944046d834a7537a496b06407dced5502943","modified":1760278603769},{"_id":"source/images/动手学day1_crossloss.png","hash":"ccab9bff7c892b5144be6b1526bab40f622471b4","modified":1760278603769},{"_id":"source/tags/index.md","hash":"2e402c99efc8c4365d94c70f83f2696122a90614","modified":1759229674327},{"_id":"source/images/隐藏层.png","hash":"71b3b6eb0f697a47709367b3767fd0dc4ae1fa3e","modified":1760278603770},{"_id":"themes/oranges/source/css/_common/layout/header.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1760279121924},{"_id":"themes/oranges/.gitignore","hash":"235a46a06c2464f3e8142d7b78d853ec621038b3","modified":1760279121919},{"_id":"themes/oranges/LICENSE","hash":"dbd3fa0b99b0acb027e8671cef34a6491e839758","modified":1760279121919},{"_id":"themes/oranges/README.md","hash":"c5ca9783e7a0351de1909af1b59e1abe2cce4a31","modified":1760279121919},{"_id":"themes/oranges/_config.yml","hash":"3513aea76bbfd34c288d342416989b73f5fbc26d","modified":1760279121920},{"_id":"themes/oranges/README-zh.md","hash":"3e9d9e066d298790df1663ea8a9bc598eddb7bcd","modified":1760279121919},{"_id":"themes/oranges/languages/de.yml","hash":"ad90132d331b9c5684129f2f08ddeaff27e1a4da","modified":1760279121920},{"_id":"themes/oranges/languages/default.yml","hash":"c0493633b1d07ec130b24ac3fd27717ffcd30731","modified":1760279121920},{"_id":"themes/oranges/languages/es.yml","hash":"a75cef476c5838772690240d23b36f334bfe147b","modified":1760279121920},{"_id":"themes/oranges/languages/fr.yml","hash":"dfc3fc2513f6d8a50babdb2076d5bcaba52339f7","modified":1760279121920},{"_id":"themes/oranges/languages/ko.yml","hash":"382182478ffa99f9a3c8d025bf7de1bb1cb06c45","modified":1760279121920},{"_id":"themes/oranges/languages/ja.yml","hash":"ebd19d2b2329767530145bc0822d1f883783049d","modified":1760279121920},{"_id":"themes/oranges/languages/nl.yml","hash":"728a17f83f604f59b52cf3fb0ce1b2434e96bf4c","modified":1760279121921},{"_id":"themes/oranges/languages/no.yml","hash":"2c2038d86738da9de2c4b3668c5e5928240380ce","modified":1760279121921},{"_id":"themes/oranges/languages/pt.yml","hash":"6d924968447e2b4a31de9ac1329326db89ab5a27","modified":1760279121921},{"_id":"themes/oranges/languages/ru.yml","hash":"509cb6b67625d5b46a38fab387d946c7a336cad4","modified":1760279121921},{"_id":"themes/oranges/languages/zh-CN.yml","hash":"133c650074fbf18e4f90bfacb34487cdbfcc9191","modified":1760279121921},{"_id":"themes/oranges/languages/zh-TW.yml","hash":"28ef519d0eccca0aa6f7637204fcdd8286532e78","modified":1760279121921},{"_id":"themes/oranges/layout/archive.ejs","hash":"ac08ee5bdb60b625896d16540ab444be844cab2b","modified":1760279121922},{"_id":"themes/oranges/layout/category.ejs","hash":"b731a87eab749d49b699af990149dfc53374f4ea","modified":1760279121923},{"_id":"themes/oranges/layout/index.ejs","hash":"9e05d7e8ca4bf38a0c36b8c1db29465b7cc561c7","modified":1760279121923},{"_id":"themes/oranges/layout/layout.ejs","hash":"a4ca62256a067680d703c0b76d9b71cdd9773f2d","modified":1760279121923},{"_id":"themes/oranges/layout/post.ejs","hash":"2dabf35adc38c640209f33eede3b9c92a2a9c6d2","modified":1760279121923},{"_id":"themes/oranges/layout/tag.ejs","hash":"209dc1ab4bfdf8123fc18a0ceefcd96787b44596","modified":1760279121923},{"_id":"themes/oranges/layout/_partial/backtotop.ejs","hash":"6b19f389755cbad990905c9284a13c70971e72b0","modified":1760279121921},{"_id":"themes/oranges/layout/_partial/clipboard.ejs","hash":"7fd17533fb4c5dd7524a381934ffcd5c27b0b6aa","modified":1760279121921},{"_id":"themes/oranges/layout/_partial/catalog.ejs","hash":"fb871254561b2b6e41d1e0195fc96684c6edc527","modified":1760279121921},{"_id":"themes/oranges/layout/_partial/comments.ejs","hash":"3e80be3f094767c672d7f34dd8611843641d6d44","modified":1760279121922},{"_id":"themes/oranges/layout/_partial/colorscheme.ejs","hash":"993af90279088a2934a857b4c51c8487e845d3e7","modified":1760279121921},{"_id":"themes/oranges/layout/_partial/footer.ejs","hash":"7e491f26c9eec689133f688a341d0bd2d3314a09","modified":1760279121922},{"_id":"themes/oranges/layout/_partial/header.ejs","hash":"929efe6f82f96ab26a4a501922036ec5085846e5","modified":1760279121922},{"_id":"themes/oranges/layout/_partial/mathjax.ejs","hash":"456981c33c943454f25fd025b76ec082f1dc3368","modified":1760279121922},{"_id":"themes/oranges/layout/_partial/navigation.ejs","hash":"449e17212e5bd98575a4c157a61cf8c801ef0e7b","modified":1760279121922},{"_id":"themes/oranges/layout/_partial/search.ejs","hash":"a6df7b553ff03b5f292571168aefe9e6dbba656a","modified":1760279121922},{"_id":"themes/oranges/source/css/color-scheme.css","hash":"b4a413e41e29e25e6472a1df2e8df36601c20174","modified":1760279121924},{"_id":"themes/oranges/layout/_partial/shares.ejs","hash":"5d50422f3949695cd28a525ef23536d632c271f7","modified":1760279121922},{"_id":"themes/oranges/source/css/base.css","hash":"d3d22be5257d03a61f619cbd192c5c7afa69b0a9","modified":1760279121924},{"_id":"themes/oranges/source/css/comments.css","hash":"09d81f9c8dd8f5a716b8622f25bcb414aeb00281","modified":1760279121924},{"_id":"themes/oranges/source/css/github-markdown.css","hash":"805b32a92605b78cdc625c8e9339f92e13dfa575","modified":1760279121925},{"_id":"themes/oranges/source/css/highlight.css","hash":"960cfd4a2f66434c36d1d72731bb2e57a3740345","modified":1760279121925},{"_id":"themes/oranges/source/images/avatar.png","hash":"642ef58c0781d0f1885775ddd349ca7af65f24b0","modified":1760279121925},{"_id":"themes/oranges/source/css/main.styl","hash":"988a14ee1cdad166c7a9c9bd58d025d5594076c1","modified":1760279121925},{"_id":"themes/oranges/source/images/favicon.png","hash":"21cf9c2e9c36c244a6542a3b6c220f13fa1a67cc","modified":1760279121925},{"_id":"themes/oranges/source/js/activeNav.js","hash":"65150cda5900eab5dc4652f9698512bafb5833e3","modified":1760279121925},{"_id":"themes/oranges/source/js/backtotop.js","hash":"1193edfd3cb032a72ba1c100be83bd459a2f63ac","modified":1760279121926},{"_id":"themes/oranges/source/js/catalog.js","hash":"3c8215aaad1ef05323b74b5f135b0bad2a3385a6","modified":1760279121926},{"_id":"themes/oranges/source/js/codeCopy.js","hash":"fd4b24a2cd985d857f6ab51853e72b623ce8765a","modified":1760279121926},{"_id":"themes/oranges/source/js/colorscheme.js","hash":"8b5626c40874c6cd39ba0fdeb3f2f3439d85da7d","modified":1760279121926},{"_id":"themes/oranges/source/js/fancybox.js","hash":"ed4c22785ec2c0764792011be19258dce3995487","modified":1760279121926},{"_id":"themes/oranges/source/js/search.js","hash":"fbac709e9a6befddbf8b31fd571b072e3de27b73","modified":1760279121926},{"_id":"themes/oranges/source/js/shares.js","hash":"838daa82d612ce632762343dbf6153343d0036f8","modified":1760279121926},{"_id":"themes/oranges/source/plugins/clipboard.min.js","hash":"d62dcb0905e038e69ff24ab9eef9e3306d45535e","modified":1760279121926},{"_id":"themes/oranges/source/plugins/gitalk.css","hash":"4c0d5510ea487b0fe63e96464ab0b381565cc273","modified":1760279121927},{"_id":"themes/oranges/source/plugins/jquery.fancybox.min.css","hash":"2e6a66987dbc7a57bbfd2655bce166739b4ba426","modified":1760279121930},{"_id":"themes/oranges/source/plugins/waline.css","hash":"25364b45ca67bc573d18bd15e252db737cfa3c0d","modified":1760279121937},{"_id":"themes/oranges/source/css/figcaption/mac-block.css","hash":"db51c58260e441632734d9bc2d41be2aa31df7c1","modified":1760279121925},{"_id":"themes/oranges/source/css/_common/comments/index.styl","hash":"7085df46c9d80aa1ea1cd361d75dfb8815e2d3da","modified":1760279121923},{"_id":"themes/oranges/source/css/_common/comments/valine.styl","hash":"23ee39dc6bb081fcb424746d1cc0c9a8cfa43d81","modified":1760279121923},{"_id":"themes/oranges/source/css/_common/components/fancybox.styl","hash":"9cf143a4215d28f851e7dd47dfa613719f4756d0","modified":1760279121923},{"_id":"themes/oranges/source/css/_common/components/index.styl","hash":"6aa40f2cfe058c7b73dcde03586c71421725f842","modified":1760279121924},{"_id":"themes/oranges/source/css/_common/utils/index.styl","hash":"cf587cb1e8cb0cc6b47efa9944094e7558a15c92","modified":1760279121924},{"_id":"themes/oranges/source/css/_common/layout/footer.styl","hash":"c83b8b135e10327f6c9779b4708bb625adc6462c","modified":1760279121924},{"_id":"themes/oranges/source/css/_common/layout/index.styl","hash":"967a2fc81e5394c95b5172680d41c48c35942995","modified":1760279121924},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff","hash":"89d2c8d274693c5a6e250e96e2a2e26e25619079","modified":1760279121932},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff","hash":"9c98f9f022647eb802434947e062b569ccedd5f0","modified":1760279121932},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff","hash":"91a4025dd3b18ca6bda63a215869773705435041","modified":1760279121932},{"_id":"themes/oranges/source/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff","hash":"6712cf0aa5c12edc1cbbca8a1732a9cde0854c48","modified":1760279121932},{"_id":"themes/oranges/source/plugins/jquery.fancybox.min.js","hash":"3154fd527a002788848d9fec61d522048890e516","modified":1760279121931},{"_id":"themes/oranges/source/plugins/jquery.min.js","hash":"0c3192b500a4fd550e483cf77a49806a5872185b","modified":1760279121931},{"_id":"themes/oranges/source/plugins/valine.min.js","hash":"c2f2b1b0346e28ceae19f4b3d174f033311aa060","modified":1760279121937},{"_id":"themes/oranges/source/plugins/waline.js","hash":"0f46a69ce82aefd07e445cb05ff6c941e83da9d1","modified":1760279121938},{"_id":"themes/oranges/source/plugins/waline.mjs","hash":"5ead58d3654d8618fc6929e862d03c787cbf21f7","modified":1760279121939},{"_id":"themes/oranges/source/plugins/gitalk.min.js","hash":"1df59d7e5481ac2917c7043b28883393675dcaf9","modified":1760279121930},{"_id":"themes/oranges/source/plugins/mathjax/tex-chtml.js","hash":"bef586271c8246d003509a68b8f11181d967847d","modified":1760279121936},{"_id":"public/search.xml","hash":"874802d2c0e4f41c5111ef3b17bd5666a83ddeed","modified":1760279142212},{"_id":"public/categories/index.html","hash":"1a5ccd109cedbcccf1dc491d2407e18d3436098f","modified":1760279142212},{"_id":"public/about/index.html","hash":"4fd9960507e6e5cc578b5646de429361bbf528d9","modified":1760279142212},{"_id":"public/friends/index.html","hash":"e00911851657d5f711057e37bc295ae6a9393651","modified":1760279142212},{"_id":"public/tags/index.html","hash":"19afdcc4514540d7a58d9f16ce8a2b9b55b7e792","modified":1760279142212},{"_id":"public/2025/10/01/动手学AI-多层感知机/index.html","hash":"700dd62c94d499ee99af5a525eb9d3b44a409aed","modified":1760279142212},{"_id":"public/2025/10/01/动手学AI-线性回归/index.html","hash":"22386ad5e30fa303ce213455abcb1140aeaf62d6","modified":1760279142212},{"_id":"public/archives/index.html","hash":"1400084503249581db65f3081d054583f216de86","modified":1760279142212},{"_id":"public/archives/2025/index.html","hash":"3bcd84f0c0083095a6ecb23dea132094654ce0bb","modified":1760279142212},{"_id":"public/archives/2025/10/index.html","hash":"79d3230a216739e00725161f0ef03a55084a066d","modified":1760279142212},{"_id":"public/categories/学习/index.html","hash":"aa9805e387d528df72bfdbabb38d797b229a658a","modified":1760279142212},{"_id":"public/index.html","hash":"a074db1bd41206991dcd179f6fcee33e810e899e","modified":1760279142212},{"_id":"public/tags/AI/index.html","hash":"c135e0fdc370e5f93640382701abed9cad8d9a47","modified":1760279142212},{"_id":"public/tags/机器学习/index.html","hash":"54c346a3d40229c98efa84601de845bfd0226a91","modified":1760279142212},{"_id":"public/images/image.png","hash":"d60f944046d834a7537a496b06407dced5502943","modified":1760279142212},{"_id":"public/images/动手学day1_crossloss.png","hash":"ccab9bff7c892b5144be6b1526bab40f622471b4","modified":1760279142212},{"_id":"public/images/avatar.png","hash":"642ef58c0781d0f1885775ddd349ca7af65f24b0","modified":1760279142212},{"_id":"public/images/favicon.png","hash":"21cf9c2e9c36c244a6542a3b6c220f13fa1a67cc","modified":1760279142212},{"_id":"public/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff","hash":"89d2c8d274693c5a6e250e96e2a2e26e25619079","modified":1760279142212},{"_id":"public/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff","hash":"9c98f9f022647eb802434947e062b569ccedd5f0","modified":1760279142212},{"_id":"public/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff","hash":"91a4025dd3b18ca6bda63a215869773705435041","modified":1760279142212},{"_id":"public/plugins/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff","hash":"6712cf0aa5c12edc1cbbca8a1732a9cde0854c48","modified":1760279142212},{"_id":"public/images/隐藏层.png","hash":"71b3b6eb0f697a47709367b3767fd0dc4ae1fa3e","modified":1760279142212},{"_id":"public/css/color-scheme.css","hash":"ec1e624f750e8caf5d463437b06bd1f705173467","modified":1760279142212},{"_id":"public/css/base.css","hash":"6fc8a265777c40336678c9b5cc033f7399b55459","modified":1760279142212},{"_id":"public/css/comments.css","hash":"b482de7b2e284ea0750aba0c0f85aca3d42c0af8","modified":1760279142212},{"_id":"public/css/github-markdown.css","hash":"0dc8e66ad2121924445150eb59a5f6091662f6f7","modified":1760279142212},{"_id":"public/css/main.css","hash":"bc42d2e3c1c705a20b96fa37313d8025c52c091e","modified":1760279142212},{"_id":"public/css/highlight.css","hash":"7ebfcbb58e87dd0436ab9538641eb6577fa7a8f8","modified":1760279142212},{"_id":"public/js/activeNav.js","hash":"06aa9a2985f1d328f0b7fe69a28bee31f16ebb1a","modified":1760279142212},{"_id":"public/js/backtotop.js","hash":"0be1bd072a7a34ce50d72376cad722023e772e6d","modified":1760279142212},{"_id":"public/js/catalog.js","hash":"3f895778af2029bff0cd588eeca0a8b64845065d","modified":1760279142212},{"_id":"public/js/codeCopy.js","hash":"3fab7bf3e0d22326440af1963e83448f8b8b4ab6","modified":1760279142212},{"_id":"public/js/colorscheme.js","hash":"1290f902b5651bf4d66187b5695ec90dc3ec70a8","modified":1760279142212},{"_id":"public/js/fancybox.js","hash":"b217d56f8db94498d7e272d164abac6ab1c07ddd","modified":1760279142212},{"_id":"public/js/search.js","hash":"374efc788268330edd7ca0c91a43e75f7ec4149c","modified":1760279142212},{"_id":"public/js/shares.js","hash":"aa0a3dd5c24efe7945351f7ac22d0f84a93e350c","modified":1760279142212},{"_id":"public/plugins/clipboard.min.js","hash":"9a7cb405f9beed005891587d41f76a0720893ffc","modified":1760279142212},{"_id":"public/plugins/gitalk.css","hash":"61d71cb30f5f34cbb1f2b5bc469784d6cb908c22","modified":1760279142212},{"_id":"public/plugins/jquery.fancybox.min.css","hash":"2e6a66987dbc7a57bbfd2655bce166739b4ba426","modified":1760279142212},{"_id":"public/plugins/jquery.fancybox.min.js","hash":"b2b093d8f5ffeee250c8d0d3a2285a213318e4ea","modified":1760279142212},{"_id":"public/plugins/waline.css","hash":"ba52d91d4685f8e07423a5651fea8a8f5151f457","modified":1760279142212},{"_id":"public/plugins/jquery.min.js","hash":"0dc32db4aa9c5f03f3b38c47d883dbd4fed13aae","modified":1760279142212},{"_id":"public/plugins/valine.min.js","hash":"d081a412c63411a75a3a880ddece65335d1c3ee8","modified":1760279142212},{"_id":"public/css/figcaption/mac-block.css","hash":"d923323312d78ecb40cb60c093dba36b0127db68","modified":1760279142212},{"_id":"public/plugins/waline.js","hash":"c26a8b22924813d883fb23e232063dce9b4e01c3","modified":1760279142212},{"_id":"public/plugins/waline.mjs","hash":"5ead58d3654d8618fc6929e862d03c787cbf21f7","modified":1760279142212},{"_id":"public/plugins/gitalk.min.js","hash":"564fc7c731d05fa70d71ef853a2c8cc7725739e2","modified":1760279142212},{"_id":"public/plugins/mathjax/tex-chtml.js","hash":"bef586271c8246d003509a68b8f11181d967847d","modified":1760279142212},{"_id":"source/images/ReLU.png","hash":"91a39ae0d58e8a294ed7532faa64bb5460405b4f","modified":1760342487675},{"_id":"source/_posts/调研-论文阅读.md","hash":"304447864b932b3e3e1a1c59adaeae02aaf674aa","modified":1761291882849},{"_id":"source/_posts/动手学AI-基本知识点.md","hash":"52ea218786ac945aae83c151376cbc41db2d8698","modified":1761102819886},{"_id":"source/images/sigmoid.png","hash":"cd6775436ccfb59d1a3fb62ff3b220a2dcbb655b","modified":1761102819891},{"_id":"source/images/sigmoid_grad.png","hash":"15ed49ae188784c26a23f17552de583b5dd3ca45","modified":1761102819893},{"_id":"source/images/human-llm-metacogiitive.png","hash":"da17bb94e270dd7ad1bd49b63afcad723821ce39","modified":1761102819890},{"_id":"source/images/metacognitive-prompt.png","hash":"6ae1935f4aeea25e0a511c3076ca172b3e0c5f1a","modified":1761288265595},{"_id":"source/images/MetaFrame.png","hash":"85eb8c0a428f088c1b4e03e47bbee59aea4d7735","modified":1761291555500}],"Category":[{"name":"学习","_id":"cmgnsr37o0006f4gf7h935gwx"},{"name":"科研","_id":"cmh4jpy0k0005xoc1gq8jbz8y"}],"Data":[],"Page":[{"title":"分类","date":"2022-02-23T09:56:00.000Z","aside":false,"top_img":false,"type":"categories","_content":"\ntest","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2022-02-23 17:56:00\naside: false\ntop_img: false\ntype: \"categories\"\n---\n\ntest","updated":"2025-09-30T11:13:00.122Z","path":"categories/index.html","_id":"cmgnsr37i0000f4gfcss7d9e1","comments":1,"layout":"page","content":"<p>test</p>\n","excerpt":"","more":"<p>test</p>\n"},{"title":"tags","date":"2025-09-29T16:00:00.000Z","type":"about","categories":null,"tags":null,"_content":"\n# 👋 Hi, 我是王亿宽\n\n欢迎来到我的博客！🎉  \n我目前就读于 **武汉大学 国家网络安全学院**，专业是 **网络空间安全**。  \n目前大四，是一名准研究生。\n\n---\n\n## 🔬 研究兴趣\n- AI safety \n- 安全 Agent 👀  \n- 深度学习与大模型安全 🤖  \n\n\n\n---\n\n## 🏆 竞赛与荣誉\n- **中国机器人及人工智能大赛** 🤖 全国一等奖 & 三等奖  \n- **全国大学生数学竞赛**（非数学A类）🏅 一等奖  \n- **全国大学生物联网设计竞赛** 📡 省部级一等奖  \n- **蓝桥杯大赛软件赛** 💻 省部级一等奖  \n- **国家励志奖学金、雷军研学基金、三好学生** 等校级荣誉  \n\n\n\n---\n\n## 💻 技能树\n- **编程语言**：Python / C++ / Go / Rust / JavaScript / Node.js / LaTeX 等  \n- **框架工具**：PyTorch / Transformers / Pandas / React / Next.js / Django / Axum  \n- **开发方向**：算法实现、Web 全栈开发、科研原型系统、机器学习工程化  \n\n\n\n---\n\n## 🌟 校园生活\n- 担任过 **院学生会主席、班长**，也做过算法课程助教 🧑‍🏫。  \n- 既能统筹大型活动，也能给同学们讲清楚一道算法题。  \n- 我相信技术和组织力是相辅相成的：写好代码，也要把人和事安排得井井有条 ✨。\n\n---\n\n\n---\n\n<!-- 📬 **联系方式**  \n- Email: 2317705863@qq.com  \n- GitHub: [OneWide](https://github.com/OneWide)   -->\n\n---\n","source":"about/index.md","raw":"---\ntitle: tags\ndate: 2025-09-30\ntype: \"about\"\ncategories:\ntags:\n---\n\n# 👋 Hi, 我是王亿宽\n\n欢迎来到我的博客！🎉  \n我目前就读于 **武汉大学 国家网络安全学院**，专业是 **网络空间安全**。  \n目前大四，是一名准研究生。\n\n---\n\n## 🔬 研究兴趣\n- AI safety \n- 安全 Agent 👀  \n- 深度学习与大模型安全 🤖  \n\n\n\n---\n\n## 🏆 竞赛与荣誉\n- **中国机器人及人工智能大赛** 🤖 全国一等奖 & 三等奖  \n- **全国大学生数学竞赛**（非数学A类）🏅 一等奖  \n- **全国大学生物联网设计竞赛** 📡 省部级一等奖  \n- **蓝桥杯大赛软件赛** 💻 省部级一等奖  \n- **国家励志奖学金、雷军研学基金、三好学生** 等校级荣誉  \n\n\n\n---\n\n## 💻 技能树\n- **编程语言**：Python / C++ / Go / Rust / JavaScript / Node.js / LaTeX 等  \n- **框架工具**：PyTorch / Transformers / Pandas / React / Next.js / Django / Axum  \n- **开发方向**：算法实现、Web 全栈开发、科研原型系统、机器学习工程化  \n\n\n\n---\n\n## 🌟 校园生活\n- 担任过 **院学生会主席、班长**，也做过算法课程助教 🧑‍🏫。  \n- 既能统筹大型活动，也能给同学们讲清楚一道算法题。  \n- 我相信技术和组织力是相辅相成的：写好代码，也要把人和事安排得井井有条 ✨。\n\n---\n\n\n---\n\n<!-- 📬 **联系方式**  \n- Email: 2317705863@qq.com  \n- GitHub: [OneWide](https://github.com/OneWide)   -->\n\n---\n","updated":"2025-10-24T07:47:25.284Z","path":"about/index.html","_id":"cmgnsr37m0002f4gffyxzck39","comments":1,"layout":"page","content":"<h1 id=\"👋-Hi-我是王亿宽\"><a href=\"#👋-Hi-我是王亿宽\" class=\"headerlink\" title=\"👋 Hi, 我是王亿宽\"></a>👋 Hi, 我是王亿宽</h1><p>欢迎来到我的博客！🎉<br>我目前就读于 <strong>武汉大学 国家网络安全学院</strong>，专业是 <strong>网络空间安全</strong>。<br>目前大四，是一名准研究生。</p>\n<hr>\n<h2 id=\"🔬-研究兴趣\"><a href=\"#🔬-研究兴趣\" class=\"headerlink\" title=\"🔬 研究兴趣\"></a>🔬 研究兴趣</h2><ul>\n<li>AI safety </li>\n<li>安全 Agent 👀  </li>\n<li>深度学习与大模型安全 🤖</li>\n</ul>\n<hr>\n<h2 id=\"🏆-竞赛与荣誉\"><a href=\"#🏆-竞赛与荣誉\" class=\"headerlink\" title=\"🏆 竞赛与荣誉\"></a>🏆 竞赛与荣誉</h2><ul>\n<li><strong>中国机器人及人工智能大赛</strong> 🤖 全国一等奖 &amp; 三等奖  </li>\n<li><strong>全国大学生数学竞赛</strong>（非数学A类）🏅 一等奖  </li>\n<li><strong>全国大学生物联网设计竞赛</strong> 📡 省部级一等奖  </li>\n<li><strong>蓝桥杯大赛软件赛</strong> 💻 省部级一等奖  </li>\n<li><strong>国家励志奖学金、雷军研学基金、三好学生</strong> 等校级荣誉</li>\n</ul>\n<hr>\n<h2 id=\"💻-技能树\"><a href=\"#💻-技能树\" class=\"headerlink\" title=\"💻 技能树\"></a>💻 技能树</h2><ul>\n<li><strong>编程语言</strong>：Python &#x2F; C++ &#x2F; Go &#x2F; Rust &#x2F; JavaScript &#x2F; Node.js &#x2F; LaTeX 等  </li>\n<li><strong>框架工具</strong>：PyTorch &#x2F; Transformers &#x2F; Pandas &#x2F; React &#x2F; Next.js &#x2F; Django &#x2F; Axum  </li>\n<li><strong>开发方向</strong>：算法实现、Web 全栈开发、科研原型系统、机器学习工程化</li>\n</ul>\n<hr>\n<h2 id=\"🌟-校园生活\"><a href=\"#🌟-校园生活\" class=\"headerlink\" title=\"🌟 校园生活\"></a>🌟 校园生活</h2><ul>\n<li>担任过 <strong>院学生会主席、班长</strong>，也做过算法课程助教 🧑‍🏫。  </li>\n<li>既能统筹大型活动，也能给同学们讲清楚一道算法题。  </li>\n<li>我相信技术和组织力是相辅相成的：写好代码，也要把人和事安排得井井有条 ✨。</li>\n</ul>\n<hr>\n<hr>\n<!-- 📬 **联系方式**  \n- Email: 2317705863@qq.com  \n- GitHub: [OneWide](https://github.com/OneWide)   -->\n\n<hr>\n","excerpt":"","more":"<h1 id=\"👋-Hi-我是王亿宽\"><a href=\"#👋-Hi-我是王亿宽\" class=\"headerlink\" title=\"👋 Hi, 我是王亿宽\"></a>👋 Hi, 我是王亿宽</h1><p>欢迎来到我的博客！🎉<br>我目前就读于 <strong>武汉大学 国家网络安全学院</strong>，专业是 <strong>网络空间安全</strong>。<br>目前大四，是一名准研究生。</p>\n<hr>\n<h2 id=\"🔬-研究兴趣\"><a href=\"#🔬-研究兴趣\" class=\"headerlink\" title=\"🔬 研究兴趣\"></a>🔬 研究兴趣</h2><ul>\n<li>AI safety </li>\n<li>安全 Agent 👀  </li>\n<li>深度学习与大模型安全 🤖</li>\n</ul>\n<hr>\n<h2 id=\"🏆-竞赛与荣誉\"><a href=\"#🏆-竞赛与荣誉\" class=\"headerlink\" title=\"🏆 竞赛与荣誉\"></a>🏆 竞赛与荣誉</h2><ul>\n<li><strong>中国机器人及人工智能大赛</strong> 🤖 全国一等奖 &amp; 三等奖  </li>\n<li><strong>全国大学生数学竞赛</strong>（非数学A类）🏅 一等奖  </li>\n<li><strong>全国大学生物联网设计竞赛</strong> 📡 省部级一等奖  </li>\n<li><strong>蓝桥杯大赛软件赛</strong> 💻 省部级一等奖  </li>\n<li><strong>国家励志奖学金、雷军研学基金、三好学生</strong> 等校级荣誉</li>\n</ul>\n<hr>\n<h2 id=\"💻-技能树\"><a href=\"#💻-技能树\" class=\"headerlink\" title=\"💻 技能树\"></a>💻 技能树</h2><ul>\n<li><strong>编程语言</strong>：Python &#x2F; C++ &#x2F; Go &#x2F; Rust &#x2F; JavaScript &#x2F; Node.js &#x2F; LaTeX 等  </li>\n<li><strong>框架工具</strong>：PyTorch &#x2F; Transformers &#x2F; Pandas &#x2F; React &#x2F; Next.js &#x2F; Django &#x2F; Axum  </li>\n<li><strong>开发方向</strong>：算法实现、Web 全栈开发、科研原型系统、机器学习工程化</li>\n</ul>\n<hr>\n<h2 id=\"🌟-校园生活\"><a href=\"#🌟-校园生活\" class=\"headerlink\" title=\"🌟 校园生活\"></a>🌟 校园生活</h2><ul>\n<li>担任过 <strong>院学生会主席、班长</strong>，也做过算法课程助教 🧑‍🏫。  </li>\n<li>既能统筹大型活动，也能给同学们讲清楚一道算法题。  </li>\n<li>我相信技术和组织力是相辅相成的：写好代码，也要把人和事安排得井井有条 ✨。</li>\n</ul>\n<hr>\n<hr>\n<!-- 📬 **联系方式**  \n- Email: 2317705863@qq.com  \n- GitHub: [OneWide](https://github.com/OneWide)   -->\n\n<hr>\n"},{"title":"tags","date":"2025-09-29T16:00:00.000Z","type":"friends","categories":null,"tags":null,"_content":"","source":"friends/index.md","raw":"---\ntitle: tags\ndate: 2025-9-30\ntype: \"friends\"\ncategories:\ntags:\n---","updated":"2025-09-30T10:55:44.365Z","path":"friends/index.html","_id":"cmgnsr37o0005f4gfeu280ovd","comments":1,"layout":"page","content":"","excerpt":"","more":""},{"title":"tags","date":"2025-09-29T16:00:00.000Z","type":"tags","categories":null,"tags":null,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2025-09-30 \ntype: \"tags\"\ncategories:\ntags:\n---","updated":"2025-09-30T10:54:34.327Z","path":"tags/index.html","_id":"cmgnsr37o0007f4gf2igt2ix9","comments":1,"layout":"page","content":"","excerpt":"","more":""}],"Post":[{"layout":"posts","title":"动手学AI_多层感知机","date":"2025-10-01T12:00:42.000Z","_content":"\n# 多层感知机是什么\n## 隐藏层\n前面说的线性模型是基于线性假设进行的，但是单纯的线性在真实世界里往往是不合理的，例如通过像素作为特征判断猫和狗，但是图片翻转后猫和狗仍不变，但像素特征改变，因此我们需要加一个或多个**隐藏层**来克服线性模型的限制，这就是**多层感知机（MLP）**\n![alt text](../images/隐藏层.png) \n如图，该多层感知机有4个输入，3个输出，5个隐藏单元，中间是全连接层。（层数为2，是全连接的，因为输入层不涉及运算）\n## 线性到非线性\n看上面的单层隐藏层的感知机，隐藏层其实就是加了一个隐藏层权重 ${W^1}$ 和隐藏层偏置${b^1}$,输出到输出层时把隐藏层的结果乘上输出层权重和偏置${W^2,b^2}$。\n\n也就是说加了隐藏层和没加差不多，毕竟我们可以通过合并隐藏层让 ${W = W^1 * W^2}$，${b = b^1 * W^2 + b^2}$来表示，那我们的多层架构就没有了任何意义，所以我们需要再仿射变换之后对每个隐藏单元应用非线性的激活函数 ${\\sigma}$，激活函数的输出称为活性值。 加入了激活函数之后，多层感知机就不会退化成线性模型。\n\n${H^{(1)} = \\sigma_1(XW^1+b^1)}$ ，${H^{(2)}=\\sigma_2(H^{(1)}W^2 +b^2)}$\n\n## 激活函数\nReLU函数，以0为活性值，仅保留正元素并丢弃所有负元素。\n\n```py\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny = torch.relu(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))\n```\n\n![alt text](../images/ReLU.png)\n该函数输入为负则导数为0，否则导数为1，输入0时不可导但使用0作为其导数，他的求导表现要么让参数消失，要么让参数通过，减轻了神经网络的梯度消失问题。\n\nsigmoid函数则是将输入变换为区间（0，1）上的输出：\n$${sigmoid(x) = \\frac{1}{1+exp(-x)}}$$\nsigmoid和我们前面学的softmax很像，可以看成是softmax的特例。但他在隐藏层中已经很少使用，一般都使用更简单的ReLU，sigmoid更多用来控制时序信息流的架构。\n\n当输入接近0时，sigmoid函数接近线性变换。\n![](../images/sigmoid.png)\n\n其导数图像如下图：\n![](../images/sigmoid_grad.png)\n当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。\n\nsigmoid图像长得跟tanh很像，只是tanh关于原点对称。\n\n# 如何实现多层感知机\n仍旧以Fashion-MNIST数据集为例，把图像视为784个特征和10个类的简单分类数据集，实现一个单隐藏层多层感知机，隐藏单元为256个。\n\n```py\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\nbatch_size = 256\ntrain_iter , test_iter = d2l.load_data_fashion_mnist(batch_size)\n\nnum_inputs,num_outputs,num_hiddens = 784,10,256\nW1 = nn.Paramter(torch.randn(\n    num_inputs,num_hiddens,requires_grad = True)*0.01)\nb1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\nW2 = nn.Parameter(torch.randn(\n    num_hiddens, num_outputs, requires_grad=True) * 0.01)\nb2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n\nparams = [W1, b1, W2, b2]\n```\n写一个ReLU函数\n```py\ndef relu(X):\n    a = torch.zeros_like(X) #与X形状相同的向量a\n    return torch.max(X, a)\n```\n网络则是直接把二维图像转换过来即可\n\n```py\ndef net(X):\n    X = X.reshape((-1,num_inputs))\n    H = relu(X@W1 + b1)\n    return (H@W2 + b2)\n```\n训练过程就比较简单，把迭代周期设置为10，学习率调整为0.1\n```py\nloss = nn.CrossEntroptLoss(reduction = 'none')\nnum_epochs, lr = 10, 0.1\nupdater = torch.optim.SGD(params, lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n```\n# 简洁实现\n与上一章写的softmax相比，我们这只是添加了两个全连接层（一个隐藏层一个输出层），并使用了ReLU激活函数。\n```py\nnet = nn.Sequential(nn.Flatten(),\n                    nn.Linear(784, 256),\n                    nn.ReLU(),\n                    nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights)\n\nbatch_size, lr, num_epochs = 256, 0.1, 10\nloss = nn.CrossEntropyLoss(reduction='none')\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\n\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```","source":"_posts/动手学AI-多层感知机.md","raw":"---\nlayout: posts\ntitle: 动手学AI_多层感知机\ndate: 2025-10-01 20:00:42\ntags: \n - [AI]\n - [机器学习]\ncategories: \n - [学习]\n---\n\n# 多层感知机是什么\n## 隐藏层\n前面说的线性模型是基于线性假设进行的，但是单纯的线性在真实世界里往往是不合理的，例如通过像素作为特征判断猫和狗，但是图片翻转后猫和狗仍不变，但像素特征改变，因此我们需要加一个或多个**隐藏层**来克服线性模型的限制，这就是**多层感知机（MLP）**\n![alt text](../images/隐藏层.png) \n如图，该多层感知机有4个输入，3个输出，5个隐藏单元，中间是全连接层。（层数为2，是全连接的，因为输入层不涉及运算）\n## 线性到非线性\n看上面的单层隐藏层的感知机，隐藏层其实就是加了一个隐藏层权重 ${W^1}$ 和隐藏层偏置${b^1}$,输出到输出层时把隐藏层的结果乘上输出层权重和偏置${W^2,b^2}$。\n\n也就是说加了隐藏层和没加差不多，毕竟我们可以通过合并隐藏层让 ${W = W^1 * W^2}$，${b = b^1 * W^2 + b^2}$来表示，那我们的多层架构就没有了任何意义，所以我们需要再仿射变换之后对每个隐藏单元应用非线性的激活函数 ${\\sigma}$，激活函数的输出称为活性值。 加入了激活函数之后，多层感知机就不会退化成线性模型。\n\n${H^{(1)} = \\sigma_1(XW^1+b^1)}$ ，${H^{(2)}=\\sigma_2(H^{(1)}W^2 +b^2)}$\n\n## 激活函数\nReLU函数，以0为活性值，仅保留正元素并丢弃所有负元素。\n\n```py\nx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny = torch.relu(x)\nd2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))\n```\n\n![alt text](../images/ReLU.png)\n该函数输入为负则导数为0，否则导数为1，输入0时不可导但使用0作为其导数，他的求导表现要么让参数消失，要么让参数通过，减轻了神经网络的梯度消失问题。\n\nsigmoid函数则是将输入变换为区间（0，1）上的输出：\n$${sigmoid(x) = \\frac{1}{1+exp(-x)}}$$\nsigmoid和我们前面学的softmax很像，可以看成是softmax的特例。但他在隐藏层中已经很少使用，一般都使用更简单的ReLU，sigmoid更多用来控制时序信息流的架构。\n\n当输入接近0时，sigmoid函数接近线性变换。\n![](../images/sigmoid.png)\n\n其导数图像如下图：\n![](../images/sigmoid_grad.png)\n当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。\n\nsigmoid图像长得跟tanh很像，只是tanh关于原点对称。\n\n# 如何实现多层感知机\n仍旧以Fashion-MNIST数据集为例，把图像视为784个特征和10个类的简单分类数据集，实现一个单隐藏层多层感知机，隐藏单元为256个。\n\n```py\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\nbatch_size = 256\ntrain_iter , test_iter = d2l.load_data_fashion_mnist(batch_size)\n\nnum_inputs,num_outputs,num_hiddens = 784,10,256\nW1 = nn.Paramter(torch.randn(\n    num_inputs,num_hiddens,requires_grad = True)*0.01)\nb1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\nW2 = nn.Parameter(torch.randn(\n    num_hiddens, num_outputs, requires_grad=True) * 0.01)\nb2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n\nparams = [W1, b1, W2, b2]\n```\n写一个ReLU函数\n```py\ndef relu(X):\n    a = torch.zeros_like(X) #与X形状相同的向量a\n    return torch.max(X, a)\n```\n网络则是直接把二维图像转换过来即可\n\n```py\ndef net(X):\n    X = X.reshape((-1,num_inputs))\n    H = relu(X@W1 + b1)\n    return (H@W2 + b2)\n```\n训练过程就比较简单，把迭代周期设置为10，学习率调整为0.1\n```py\nloss = nn.CrossEntroptLoss(reduction = 'none')\nnum_epochs, lr = 10, 0.1\nupdater = torch.optim.SGD(params, lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n```\n# 简洁实现\n与上一章写的softmax相比，我们这只是添加了两个全连接层（一个隐藏层一个输出层），并使用了ReLU激活函数。\n```py\nnet = nn.Sequential(nn.Flatten(),\n                    nn.Linear(784, 256),\n                    nn.ReLU(),\n                    nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights)\n\nbatch_size, lr, num_epochs = 256, 0.1, 10\nloss = nn.CrossEntropyLoss(reduction='none')\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\n\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```","slug":"动手学AI-多层感知机","published":1,"updated":"2025-10-22T03:13:39.887Z","_id":"cmgnsr37j0001f4gfbeo357xb","comments":1,"photos":[],"content":"<h1 id=\"多层感知机是什么\"><a href=\"#多层感知机是什么\" class=\"headerlink\" title=\"多层感知机是什么\"></a>多层感知机是什么</h1><h2 id=\"隐藏层\"><a href=\"#隐藏层\" class=\"headerlink\" title=\"隐藏层\"></a>隐藏层</h2><p>前面说的线性模型是基于线性假设进行的，但是单纯的线性在真实世界里往往是不合理的，例如通过像素作为特征判断猫和狗，但是图片翻转后猫和狗仍不变，但像素特征改变，因此我们需要加一个或多个<strong>隐藏层</strong>来克服线性模型的限制，这就是<strong>多层感知机（MLP）</strong><br><img src=\"/../images/%E9%9A%90%E8%97%8F%E5%B1%82.png\" alt=\"alt text\"><br>如图，该多层感知机有4个输入，3个输出，5个隐藏单元，中间是全连接层。（层数为2，是全连接的，因为输入层不涉及运算）</p>\n<h2 id=\"线性到非线性\"><a href=\"#线性到非线性\" class=\"headerlink\" title=\"线性到非线性\"></a>线性到非线性</h2><p>看上面的单层隐藏层的感知机，隐藏层其实就是加了一个隐藏层权重 ${W^1}$ 和隐藏层偏置${b^1}$,输出到输出层时把隐藏层的结果乘上输出层权重和偏置${W^2,b^2}$。</p>\n<p>也就是说加了隐藏层和没加差不多，毕竟我们可以通过合并隐藏层让 ${W &#x3D; W^1 * W^2}$，${b &#x3D; b^1 * W^2 + b^2}$来表示，那我们的多层架构就没有了任何意义，所以我们需要再仿射变换之后对每个隐藏单元应用非线性的激活函数 ${\\sigma}$，激活函数的输出称为活性值。 加入了激活函数之后，多层感知机就不会退化成线性模型。</p>\n<p>${H^{(1)} &#x3D; \\sigma_1(XW^1+b^1)}$ ，${H^{(2)}&#x3D;\\sigma_2(H^{(1)}W^2 +b^2)}$</p>\n<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><p>ReLU函数，以0为活性值，仅保留正元素并丢弃所有负元素。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(-<span class=\"number\">8.0</span>, <span class=\"number\">8.0</span>, <span class=\"number\">0.1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = torch.relu(x)</span><br><span class=\"line\">d2l.plot(x.detach(), y.detach(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;relu(x)&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/../images/ReLU.png\" alt=\"alt text\"><br>该函数输入为负则导数为0，否则导数为1，输入0时不可导但使用0作为其导数，他的求导表现要么让参数消失，要么让参数通过，减轻了神经网络的梯度消失问题。</p>\n<p>sigmoid函数则是将输入变换为区间（0，1）上的输出：<br>$${sigmoid(x) &#x3D; \\frac{1}{1+exp(-x)}}$$<br>sigmoid和我们前面学的softmax很像，可以看成是softmax的特例。但他在隐藏层中已经很少使用，一般都使用更简单的ReLU，sigmoid更多用来控制时序信息流的架构。</p>\n<p>当输入接近0时，sigmoid函数接近线性变换。<br><img src=\"/../images/sigmoid.png\"></p>\n<p>其导数图像如下图：<br><img src=\"/../images/sigmoid_grad.png\"><br>当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。</p>\n<p>sigmoid图像长得跟tanh很像，只是tanh关于原点对称。</p>\n<h1 id=\"如何实现多层感知机\"><a href=\"#如何实现多层感知机\" class=\"headerlink\" title=\"如何实现多层感知机\"></a>如何实现多层感知机</h1><p>仍旧以Fashion-MNIST数据集为例，把图像视为784个特征和10个类的简单分类数据集，实现一个单隐藏层多层感知机，隐藏单元为256个。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter , test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\">num_inputs,num_outputs,num_hiddens = <span class=\"number\">784</span>,<span class=\"number\">10</span>,<span class=\"number\">256</span></span><br><span class=\"line\">W1 = nn.Paramter(torch.randn(</span><br><span class=\"line\">    num_inputs,num_hiddens,requires_grad = <span class=\"literal\">True</span>)*<span class=\"number\">0.01</span>)</span><br><span class=\"line\">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\">W2 = nn.Parameter(torch.randn(</span><br><span class=\"line\">    num_hiddens, num_outputs, requires_grad=<span class=\"literal\">True</span>) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">params = [W1, b1, W2, b2]</span><br></pre></td></tr></table></figure>\n<p>写一个ReLU函数</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">relu</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    a = torch.zeros_like(X) <span class=\"comment\">#与X形状相同的向量a</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">max</span>(X, a)</span><br></pre></td></tr></table></figure>\n<p>网络则是直接把二维图像转换过来即可</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    X = X.reshape((-<span class=\"number\">1</span>,num_inputs))</span><br><span class=\"line\">    H = relu(X@W1 + b1)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (H@W2 + b2)</span><br></pre></td></tr></table></figure>\n<p>训练过程就比较简单，把迭代周期设置为10，学习率调整为0.1</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss = nn.CrossEntroptLoss(reduction = <span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">num_epochs, lr = <span class=\"number\">10</span>, <span class=\"number\">0.1</span></span><br><span class=\"line\">updater = torch.optim.SGD(params, lr=lr)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure>\n<h1 id=\"简洁实现\"><a href=\"#简洁实现\" class=\"headerlink\" title=\"简洁实现\"></a>简洁实现</h1><p>与上一章写的softmax相比，我们这只是添加了两个全连接层（一个隐藏层一个输出层），并使用了ReLU激活函数。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Flatten(),</span><br><span class=\"line\">                    nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">                    nn.ReLU(),</span><br><span class=\"line\">                    nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights)</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, lr, num_epochs = <span class=\"number\">256</span>, <span class=\"number\">0.1</span>, <span class=\"number\">10</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\"></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<h1 id=\"多层感知机是什么\"><a href=\"#多层感知机是什么\" class=\"headerlink\" title=\"多层感知机是什么\"></a>多层感知机是什么</h1><h2 id=\"隐藏层\"><a href=\"#隐藏层\" class=\"headerlink\" title=\"隐藏层\"></a>隐藏层</h2><p>前面说的线性模型是基于线性假设进行的，但是单纯的线性在真实世界里往往是不合理的，例如通过像素作为特征判断猫和狗，但是图片翻转后猫和狗仍不变，但像素特征改变，因此我们需要加一个或多个<strong>隐藏层</strong>来克服线性模型的限制，这就是<strong>多层感知机（MLP）</strong><br><img src=\"/../images/%E9%9A%90%E8%97%8F%E5%B1%82.png\" alt=\"alt text\"><br>如图，该多层感知机有4个输入，3个输出，5个隐藏单元，中间是全连接层。（层数为2，是全连接的，因为输入层不涉及运算）</p>\n<h2 id=\"线性到非线性\"><a href=\"#线性到非线性\" class=\"headerlink\" title=\"线性到非线性\"></a>线性到非线性</h2><p>看上面的单层隐藏层的感知机，隐藏层其实就是加了一个隐藏层权重 ${W^1}$ 和隐藏层偏置${b^1}$,输出到输出层时把隐藏层的结果乘上输出层权重和偏置${W^2,b^2}$。</p>\n<p>也就是说加了隐藏层和没加差不多，毕竟我们可以通过合并隐藏层让 ${W &#x3D; W^1 * W^2}$，${b &#x3D; b^1 * W^2 + b^2}$来表示，那我们的多层架构就没有了任何意义，所以我们需要再仿射变换之后对每个隐藏单元应用非线性的激活函数 ${\\sigma}$，激活函数的输出称为活性值。 加入了激活函数之后，多层感知机就不会退化成线性模型。</p>\n<p>${H^{(1)} &#x3D; \\sigma_1(XW^1+b^1)}$ ，${H^{(2)}&#x3D;\\sigma_2(H^{(1)}W^2 +b^2)}$</p>\n<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><p>ReLU函数，以0为活性值，仅保留正元素并丢弃所有负元素。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(-<span class=\"number\">8.0</span>, <span class=\"number\">8.0</span>, <span class=\"number\">0.1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = torch.relu(x)</span><br><span class=\"line\">d2l.plot(x.detach(), y.detach(), <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;relu(x)&#x27;</span>, figsize=(<span class=\"number\">5</span>, <span class=\"number\">2.5</span>))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"/../images/ReLU.png\" alt=\"alt text\"><br>该函数输入为负则导数为0，否则导数为1，输入0时不可导但使用0作为其导数，他的求导表现要么让参数消失，要么让参数通过，减轻了神经网络的梯度消失问题。</p>\n<p>sigmoid函数则是将输入变换为区间（0，1）上的输出：<br>$${sigmoid(x) &#x3D; \\frac{1}{1+exp(-x)}}$$<br>sigmoid和我们前面学的softmax很像，可以看成是softmax的特例。但他在隐藏层中已经很少使用，一般都使用更简单的ReLU，sigmoid更多用来控制时序信息流的架构。</p>\n<p>当输入接近0时，sigmoid函数接近线性变换。<br><img src=\"/../images/sigmoid.png\"></p>\n<p>其导数图像如下图：<br><img src=\"/../images/sigmoid_grad.png\"><br>当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。</p>\n<p>sigmoid图像长得跟tanh很像，只是tanh关于原点对称。</p>\n<h1 id=\"如何实现多层感知机\"><a href=\"#如何实现多层感知机\" class=\"headerlink\" title=\"如何实现多层感知机\"></a>如何实现多层感知机</h1><p>仍旧以Fashion-MNIST数据集为例，把图像视为784个特征和10个类的简单分类数据集，实现一个单隐藏层多层感知机，隐藏单元为256个。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter , test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\"></span><br><span class=\"line\">num_inputs,num_outputs,num_hiddens = <span class=\"number\">784</span>,<span class=\"number\">10</span>,<span class=\"number\">256</span></span><br><span class=\"line\">W1 = nn.Paramter(torch.randn(</span><br><span class=\"line\">    num_inputs,num_hiddens,requires_grad = <span class=\"literal\">True</span>)*<span class=\"number\">0.01</span>)</span><br><span class=\"line\">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\">W2 = nn.Parameter(torch.randn(</span><br><span class=\"line\">    num_hiddens, num_outputs, requires_grad=<span class=\"literal\">True</span>) * <span class=\"number\">0.01</span>)</span><br><span class=\"line\">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">params = [W1, b1, W2, b2]</span><br></pre></td></tr></table></figure>\n<p>写一个ReLU函数</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">relu</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    a = torch.zeros_like(X) <span class=\"comment\">#与X形状相同的向量a</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">max</span>(X, a)</span><br></pre></td></tr></table></figure>\n<p>网络则是直接把二维图像转换过来即可</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    X = X.reshape((-<span class=\"number\">1</span>,num_inputs))</span><br><span class=\"line\">    H = relu(X@W1 + b1)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (H@W2 + b2)</span><br></pre></td></tr></table></figure>\n<p>训练过程就比较简单，把迭代周期设置为10，学习率调整为0.1</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss = nn.CrossEntroptLoss(reduction = <span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">num_epochs, lr = <span class=\"number\">10</span>, <span class=\"number\">0.1</span></span><br><span class=\"line\">updater = torch.optim.SGD(params, lr=lr)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure>\n<h1 id=\"简洁实现\"><a href=\"#简洁实现\" class=\"headerlink\" title=\"简洁实现\"></a>简洁实现</h1><p>与上一章写的softmax相比，我们这只是添加了两个全连接层（一个隐藏层一个输出层），并使用了ReLU激活函数。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Flatten(),</span><br><span class=\"line\">                    nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">                    nn.ReLU(),</span><br><span class=\"line\">                    nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights)</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size, lr, num_epochs = <span class=\"number\">256</span>, <span class=\"number\">0.1</span>, <span class=\"number\">10</span></span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class=\"line\"></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>"},{"title":"动手学AI_线性回归","date":"2025-10-01T02:36:35.000Z","_content":"# Softmax回归\n## 从零实现\n### 初始化\n这里使用的Fashion-MNIST数据集图像为28*28，文中视为一个展平的向量，把每个像素看作一个特征。\n\nSoftmax的输出与判定的类别是**一样多的**，因此构筑成784\\*10的矩阵，偏置${b}$应该是1\\*10的向量，把权重W用正态分布初始化，把偏置用0初始化。\n\n``` python\nnum_inputs = 784 #输出维度\nnum_outputs = 10 #输入维度\n\nW = torch.normal(0,0.01,size=(num_inputs,num_outputs),requires_grad = True)\nb = torch.zeros(num_outputs,requires_grad = True)\n```\n> normal用于使用正态分布标准化变量，规定向量维度，requires_grad表示需要记录梯度，便于反向传播求导\n\n### 定义Softmax\n\nSoftmax由三步组成：\n- 对每个项求幂\n- 对每一行求和，得到每个样本的规范化常数\n- 对每一行除以规范化常数，确保结果和为1\n  \n \n\n![alt text](../images/image.png)\n\n\n于是我们根据上述步骤规定：\n```python\ndef softmax(X):\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepim = True) #轴1是行，0是列\n    return X_exp / partition\n```\n\n### 定义模型 \n模型，对应的就是输入如何通过映射到输出，这里我们是把像素点当作特征，那么我们就用reshape将原始图像转为向量。\n\n```py\ndef net(X):\n    return softmax(torch.matul(X.reshape((-1,W.shape[0]),W)+b))\n```\n\n### 定义损失函数\n使用交叉熵损失函数来进行定义损失。我们先创建一个数据样本y_hat，包含2个样本在3个类别的预测概率，以及对应的标签y。我们规定标签y在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。然后使用y作为y_hat中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。\n\n```python\ny = torch.tensor([0, 2])\ny_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ny_hat[[0, 1], y]\n```\n>这里用到了高级索引，y_hat[[0, 1], y] 相当于“从第0行取第 y[0] 个元素，从第1行取第 y[1] 个元素”\n\n下面我们定义交叉熵损失：\n\n```py\ndef cross_entropy(y_hat, y):\n    return - torch.log(y_hat[range(len(y_hat)), y])\n```\n交叉熵损失为：\n![alt text](../images/动手学day1_crossloss.png)\n\n### 分类精度\n我们上面已经给出了y_hat的预测分类，下面要看分类的精度如何。\n\n```py\ndef accuracy(y_hat, y):  \n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: #如果是高纬，就取最大值作为下标判断类别\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y #比较是否相等\n    return float(cmp.type(y.dtype).sum()) #返回相等的个数\n```\n下面判断在整个数据集上评估模型的准确率：\n```py\ndef evaluate_accuracy(net,data_iter):\n    if isinstance(net,torch.nn.Moudle):\n        net.eval() #把模型设置为评估模式，关闭dropout，batchnorm等训练行为\n    metric = Accumlator(2) #正确预测数、预测总数\n    with torch.no_grad():\n        for X,y in data_iter:\n            metric.add(accuracy(net(X),y),y.numel())\n    return metric[0] / metric[1]\n```\n这里的Accumulator是\n```py\nclass Accumulator:  #@save\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n```\n用于对多个变量进行累加,我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。\n\n### 训练\n```py\ndef train_epoch_ch3(net, train_iter, loss, updater):  #@save\n    \"\"\"训练模型一个迭代周期（定义见第3章）\"\"\"\n    # 将模型设置为训练模式\n    if isinstance(net, torch.nn.Module):\n        net.train()\n    # 训练损失总和、训练准确度总和、样本数\n    metric = Accumulator(3)\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        if isinstance(updater, torch.optim.Optimizer):\n            # 使用PyTorch内置的优化器和损失函数\n            updater.zero_grad()\n            l.mean().backward()\n            updater.step()\n        else:\n            # 使用定制的优化器和损失函数\n            l.sum().backward()\n            updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练精度\n    return metric[0] / metric[2], metric[1] / metric[2]\n```\n\n## 简洁实现\n```py\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n# PyTorch不会隐式地调整输入的形状。因此，\n# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状\nnet = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\nloss = nn.CrossEntropyLoss(reduction='none')\ntrainer = torch.optim.SGD(net.parameters(), lr=0.1)\nnum_epochs = 10\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```","source":"_posts/动手学AI-线性回归.md","raw":"---\ntitle: 动手学AI_线性回归\ndate: 2025-10-01 10:36:35\ntags: \n - [AI]\n - [机器学习]\ncategories: \n - [学习]\n---\n# Softmax回归\n## 从零实现\n### 初始化\n这里使用的Fashion-MNIST数据集图像为28*28，文中视为一个展平的向量，把每个像素看作一个特征。\n\nSoftmax的输出与判定的类别是**一样多的**，因此构筑成784\\*10的矩阵，偏置${b}$应该是1\\*10的向量，把权重W用正态分布初始化，把偏置用0初始化。\n\n``` python\nnum_inputs = 784 #输出维度\nnum_outputs = 10 #输入维度\n\nW = torch.normal(0,0.01,size=(num_inputs,num_outputs),requires_grad = True)\nb = torch.zeros(num_outputs,requires_grad = True)\n```\n> normal用于使用正态分布标准化变量，规定向量维度，requires_grad表示需要记录梯度，便于反向传播求导\n\n### 定义Softmax\n\nSoftmax由三步组成：\n- 对每个项求幂\n- 对每一行求和，得到每个样本的规范化常数\n- 对每一行除以规范化常数，确保结果和为1\n  \n \n\n![alt text](../images/image.png)\n\n\n于是我们根据上述步骤规定：\n```python\ndef softmax(X):\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepim = True) #轴1是行，0是列\n    return X_exp / partition\n```\n\n### 定义模型 \n模型，对应的就是输入如何通过映射到输出，这里我们是把像素点当作特征，那么我们就用reshape将原始图像转为向量。\n\n```py\ndef net(X):\n    return softmax(torch.matul(X.reshape((-1,W.shape[0]),W)+b))\n```\n\n### 定义损失函数\n使用交叉熵损失函数来进行定义损失。我们先创建一个数据样本y_hat，包含2个样本在3个类别的预测概率，以及对应的标签y。我们规定标签y在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。然后使用y作为y_hat中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。\n\n```python\ny = torch.tensor([0, 2])\ny_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ny_hat[[0, 1], y]\n```\n>这里用到了高级索引，y_hat[[0, 1], y] 相当于“从第0行取第 y[0] 个元素，从第1行取第 y[1] 个元素”\n\n下面我们定义交叉熵损失：\n\n```py\ndef cross_entropy(y_hat, y):\n    return - torch.log(y_hat[range(len(y_hat)), y])\n```\n交叉熵损失为：\n![alt text](../images/动手学day1_crossloss.png)\n\n### 分类精度\n我们上面已经给出了y_hat的预测分类，下面要看分类的精度如何。\n\n```py\ndef accuracy(y_hat, y):  \n    \"\"\"计算预测正确的数量\"\"\"\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: #如果是高纬，就取最大值作为下标判断类别\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y #比较是否相等\n    return float(cmp.type(y.dtype).sum()) #返回相等的个数\n```\n下面判断在整个数据集上评估模型的准确率：\n```py\ndef evaluate_accuracy(net,data_iter):\n    if isinstance(net,torch.nn.Moudle):\n        net.eval() #把模型设置为评估模式，关闭dropout，batchnorm等训练行为\n    metric = Accumlator(2) #正确预测数、预测总数\n    with torch.no_grad():\n        for X,y in data_iter:\n            metric.add(accuracy(net(X),y),y.numel())\n    return metric[0] / metric[1]\n```\n这里的Accumulator是\n```py\nclass Accumulator:  #@save\n    \"\"\"在n个变量上累加\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n```\n用于对多个变量进行累加,我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。\n\n### 训练\n```py\ndef train_epoch_ch3(net, train_iter, loss, updater):  #@save\n    \"\"\"训练模型一个迭代周期（定义见第3章）\"\"\"\n    # 将模型设置为训练模式\n    if isinstance(net, torch.nn.Module):\n        net.train()\n    # 训练损失总和、训练准确度总和、样本数\n    metric = Accumulator(3)\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        if isinstance(updater, torch.optim.Optimizer):\n            # 使用PyTorch内置的优化器和损失函数\n            updater.zero_grad()\n            l.mean().backward()\n            updater.step()\n        else:\n            # 使用定制的优化器和损失函数\n            l.sum().backward()\n            updater(X.shape[0])\n        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练精度\n    return metric[0] / metric[2], metric[1] / metric[2]\n```\n\n## 简洁实现\n```py\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n# PyTorch不会隐式地调整输入的形状。因此，\n# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状\nnet = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\nloss = nn.CrossEntropyLoss(reduction='none')\ntrainer = torch.optim.SGD(net.parameters(), lr=0.1)\nnum_epochs = 10\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```","slug":"动手学AI-线性回归","published":1,"updated":"2025-10-01T12:02:17.922Z","_id":"cmgnsr37m0003f4gf0tij76ty","comments":1,"layout":"post","photos":[],"content":"<h1 id=\"Softmax回归\"><a href=\"#Softmax回归\" class=\"headerlink\" title=\"Softmax回归\"></a>Softmax回归</h1><h2 id=\"从零实现\"><a href=\"#从零实现\" class=\"headerlink\" title=\"从零实现\"></a>从零实现</h2><h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>这里使用的Fashion-MNIST数据集图像为28*28，文中视为一个展平的向量，把每个像素看作一个特征。</p>\n<p>Softmax的输出与判定的类别是<strong>一样多的</strong>，因此构筑成784*10的矩阵，偏置${b}$应该是1*10的向量，把权重W用正态分布初始化，把偏置用0初始化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs = <span class=\"number\">784</span> <span class=\"comment\">#输出维度</span></span><br><span class=\"line\">num_outputs = <span class=\"number\">10</span> <span class=\"comment\">#输入维度</span></span><br><span class=\"line\"></span><br><span class=\"line\">W = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">0.01</span>,size=(num_inputs,num_outputs),requires_grad = <span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(num_outputs,requires_grad = <span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>normal用于使用正态分布标准化变量，规定向量维度，requires_grad表示需要记录梯度，便于反向传播求导</p>\n</blockquote>\n<h3 id=\"定义Softmax\"><a href=\"#定义Softmax\" class=\"headerlink\" title=\"定义Softmax\"></a>定义Softmax</h3><p>Softmax由三步组成：</p>\n<ul>\n<li>对每个项求幂</li>\n<li>对每一行求和，得到每个样本的规范化常数</li>\n<li>对每一行除以规范化常数，确保结果和为1</li>\n</ul>\n<p><img src=\"/../images/image.png\" alt=\"alt text\"></p>\n<p>于是我们根据上述步骤规定：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    X_exp = torch.exp(X)</span><br><span class=\"line\">    partition = X_exp.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>, keepim = <span class=\"literal\">True</span>) <span class=\"comment\">#轴1是行，0是列</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_exp / partition</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"定义模型\"><a href=\"#定义模型\" class=\"headerlink\" title=\"定义模型\"></a>定义模型</h3><p>模型，对应的就是输入如何通过映射到输出，这里我们是把像素点当作特征，那么我们就用reshape将原始图像转为向量。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> softmax(torch.matul(X.reshape((-<span class=\"number\">1</span>,W.shape[<span class=\"number\">0</span>]),W)+b))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"定义损失函数\"><a href=\"#定义损失函数\" class=\"headerlink\" title=\"定义损失函数\"></a>定义损失函数</h3><p>使用交叉熵损失函数来进行定义损失。我们先创建一个数据样本y_hat，包含2个样本在3个类别的预测概率，以及对应的标签y。我们规定标签y在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。然后使用y作为y_hat中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.tensor([<span class=\"number\">0</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">y_hat = torch.tensor([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.6</span>], [<span class=\"number\">0.3</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>]])</span><br><span class=\"line\">y_hat[[<span class=\"number\">0</span>, <span class=\"number\">1</span>], y]</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>这里用到了高级索引，y_hat[[0, 1], y] 相当于“从第0行取第 y[0] 个元素，从第1行取第 y[1] 个元素”</p>\n</blockquote>\n<p>下面我们定义交叉熵损失：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cross_entropy</span>(<span class=\"params\">y_hat, y</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> - torch.log(y_hat[<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(y_hat)), y])</span><br></pre></td></tr></table></figure>\n<p>交叉熵损失为：<br><img src=\"/../images/%E5%8A%A8%E6%89%8B%E5%AD%A6day1_crossloss.png\" alt=\"alt text\"></p>\n<h3 id=\"分类精度\"><a href=\"#分类精度\" class=\"headerlink\" title=\"分类精度\"></a>分类精度</h3><p>我们上面已经给出了y_hat的预测分类，下面要看分类的精度如何。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_hat, y</span>):  </span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(y_hat.shape) &gt; <span class=\"number\">1</span> <span class=\"keyword\">and</span> y_hat.shape[<span class=\"number\">1</span>] &gt; <span class=\"number\">1</span>: <span class=\"comment\">#如果是高纬，就取最大值作为下标判断类别</span></span><br><span class=\"line\">        y_hat = y_hat.argmax(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    cmp = y_hat.<span class=\"built_in\">type</span>(y.dtype) == y <span class=\"comment\">#比较是否相等</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">float</span>(cmp.<span class=\"built_in\">type</span>(y.dtype).<span class=\"built_in\">sum</span>()) <span class=\"comment\">#返回相等的个数</span></span><br></pre></td></tr></table></figure>\n<p>下面判断在整个数据集上评估模型的准确率：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_accuracy</span>(<span class=\"params\">net,data_iter</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net,torch.nn.Moudle):</span><br><span class=\"line\">        net.<span class=\"built_in\">eval</span>() <span class=\"comment\">#把模型设置为评估模式，关闭dropout，batchnorm等训练行为</span></span><br><span class=\"line\">    metric = Accumlator(<span class=\"number\">2</span>) <span class=\"comment\">#正确预测数、预测总数</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            metric.add(accuracy(net(X),y),y.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>\n<p>这里的Accumulator是</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Accumulator</span>:  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, n</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [<span class=\"number\">0.0</span>] * n</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">add</span>(<span class=\"params\">self, *args</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [a + <span class=\"built_in\">float</span>(b) <span class=\"keyword\">for</span> a, b <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(<span class=\"variable language_\">self</span>.data, args)]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [<span class=\"number\">0.0</span>] * <span class=\"built_in\">len</span>(<span class=\"variable language_\">self</span>.data)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.data[idx]</span><br></pre></td></tr></table></figure>\n<p>用于对多个变量进行累加,我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch_ch3</span>(<span class=\"params\">net, train_iter, loss, updater</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 将模型设置为训练模式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, torch.nn.Module):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">    <span class=\"comment\"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class=\"line\">    metric = Accumulator(<span class=\"number\">3</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">        <span class=\"comment\"># 计算梯度并更新参数</span></span><br><span class=\"line\">        y_hat = net(X)</span><br><span class=\"line\">        l = loss(y_hat, y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class=\"line\">            <span class=\"comment\"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class=\"line\">            updater.zero_grad()</span><br><span class=\"line\">            l.mean().backward()</span><br><span class=\"line\">            updater.step()</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用定制的优化器和损失函数</span></span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            updater(X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">        metric.add(<span class=\"built_in\">float</span>(l.<span class=\"built_in\">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class=\"line\">    <span class=\"comment\"># 返回训练损失和训练精度</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>], metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">2</span>]</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"简洁实现\"><a href=\"#简洁实现\" class=\"headerlink\" title=\"简洁实现\"></a>简洁实现</h2><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\"><span class=\"comment\"># PyTorch不会隐式地调整输入的形状。因此，</span></span><br><span class=\"line\"><span class=\"comment\"># 我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights);</span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>","excerpt":"","more":"<h1 id=\"Softmax回归\"><a href=\"#Softmax回归\" class=\"headerlink\" title=\"Softmax回归\"></a>Softmax回归</h1><h2 id=\"从零实现\"><a href=\"#从零实现\" class=\"headerlink\" title=\"从零实现\"></a>从零实现</h2><h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>这里使用的Fashion-MNIST数据集图像为28*28，文中视为一个展平的向量，把每个像素看作一个特征。</p>\n<p>Softmax的输出与判定的类别是<strong>一样多的</strong>，因此构筑成784*10的矩阵，偏置${b}$应该是1*10的向量，把权重W用正态分布初始化，把偏置用0初始化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs = <span class=\"number\">784</span> <span class=\"comment\">#输出维度</span></span><br><span class=\"line\">num_outputs = <span class=\"number\">10</span> <span class=\"comment\">#输入维度</span></span><br><span class=\"line\"></span><br><span class=\"line\">W = torch.normal(<span class=\"number\">0</span>,<span class=\"number\">0.01</span>,size=(num_inputs,num_outputs),requires_grad = <span class=\"literal\">True</span>)</span><br><span class=\"line\">b = torch.zeros(num_outputs,requires_grad = <span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>normal用于使用正态分布标准化变量，规定向量维度，requires_grad表示需要记录梯度，便于反向传播求导</p>\n</blockquote>\n<h3 id=\"定义Softmax\"><a href=\"#定义Softmax\" class=\"headerlink\" title=\"定义Softmax\"></a>定义Softmax</h3><p>Softmax由三步组成：</p>\n<ul>\n<li>对每个项求幂</li>\n<li>对每一行求和，得到每个样本的规范化常数</li>\n<li>对每一行除以规范化常数，确保结果和为1</li>\n</ul>\n<p><img src=\"/../images/image.png\" alt=\"alt text\"></p>\n<p>于是我们根据上述步骤规定：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">softmax</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    X_exp = torch.exp(X)</span><br><span class=\"line\">    partition = X_exp.<span class=\"built_in\">sum</span>(<span class=\"number\">1</span>, keepim = <span class=\"literal\">True</span>) <span class=\"comment\">#轴1是行，0是列</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_exp / partition</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"定义模型\"><a href=\"#定义模型\" class=\"headerlink\" title=\"定义模型\"></a>定义模型</h3><p>模型，对应的就是输入如何通过映射到输出，这里我们是把像素点当作特征，那么我们就用reshape将原始图像转为向量。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">net</span>(<span class=\"params\">X</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> softmax(torch.matul(X.reshape((-<span class=\"number\">1</span>,W.shape[<span class=\"number\">0</span>]),W)+b))</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"定义损失函数\"><a href=\"#定义损失函数\" class=\"headerlink\" title=\"定义损失函数\"></a>定义损失函数</h3><p>使用交叉熵损失函数来进行定义损失。我们先创建一个数据样本y_hat，包含2个样本在3个类别的预测概率，以及对应的标签y。我们规定标签y在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。然后使用y作为y_hat中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y = torch.tensor([<span class=\"number\">0</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">y_hat = torch.tensor([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.6</span>], [<span class=\"number\">0.3</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>]])</span><br><span class=\"line\">y_hat[[<span class=\"number\">0</span>, <span class=\"number\">1</span>], y]</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>这里用到了高级索引，y_hat[[0, 1], y] 相当于“从第0行取第 y[0] 个元素，从第1行取第 y[1] 个元素”</p>\n</blockquote>\n<p>下面我们定义交叉熵损失：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cross_entropy</span>(<span class=\"params\">y_hat, y</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> - torch.log(y_hat[<span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(y_hat)), y])</span><br></pre></td></tr></table></figure>\n<p>交叉熵损失为：<br><img src=\"/../images/%E5%8A%A8%E6%89%8B%E5%AD%A6day1_crossloss.png\" alt=\"alt text\"></p>\n<h3 id=\"分类精度\"><a href=\"#分类精度\" class=\"headerlink\" title=\"分类精度\"></a>分类精度</h3><p>我们上面已经给出了y_hat的预测分类，下面要看分类的精度如何。</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">accuracy</span>(<span class=\"params\">y_hat, y</span>):  </span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(y_hat.shape) &gt; <span class=\"number\">1</span> <span class=\"keyword\">and</span> y_hat.shape[<span class=\"number\">1</span>] &gt; <span class=\"number\">1</span>: <span class=\"comment\">#如果是高纬，就取最大值作为下标判断类别</span></span><br><span class=\"line\">        y_hat = y_hat.argmax(axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">    cmp = y_hat.<span class=\"built_in\">type</span>(y.dtype) == y <span class=\"comment\">#比较是否相等</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">float</span>(cmp.<span class=\"built_in\">type</span>(y.dtype).<span class=\"built_in\">sum</span>()) <span class=\"comment\">#返回相等的个数</span></span><br></pre></td></tr></table></figure>\n<p>下面判断在整个数据集上评估模型的准确率：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">evaluate_accuracy</span>(<span class=\"params\">net,data_iter</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net,torch.nn.Moudle):</span><br><span class=\"line\">        net.<span class=\"built_in\">eval</span>() <span class=\"comment\">#把模型设置为评估模式，关闭dropout，batchnorm等训练行为</span></span><br><span class=\"line\">    metric = Accumlator(<span class=\"number\">2</span>) <span class=\"comment\">#正确预测数、预测总数</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X,y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            metric.add(accuracy(net(X),y),y.numel())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>\n<p>这里的Accumulator是</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Accumulator</span>:  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, n</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [<span class=\"number\">0.0</span>] * n</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">add</span>(<span class=\"params\">self, *args</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [a + <span class=\"built_in\">float</span>(b) <span class=\"keyword\">for</span> a, b <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(<span class=\"variable language_\">self</span>.data, args)]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">reset</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.data = [<span class=\"number\">0.0</span>] * <span class=\"built_in\">len</span>(<span class=\"variable language_\">self</span>.data)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__getitem__</span>(<span class=\"params\">self, idx</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"variable language_\">self</span>.data[idx]</span><br></pre></td></tr></table></figure>\n<p>用于对多个变量进行累加,我们在Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。</p>\n<h3 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h3><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_epoch_ch3</span>(<span class=\"params\">net, train_iter, loss, updater</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 将模型设置为训练模式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(net, torch.nn.Module):</span><br><span class=\"line\">        net.train()</span><br><span class=\"line\">    <span class=\"comment\"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class=\"line\">    metric = Accumulator(<span class=\"number\">3</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">        <span class=\"comment\"># 计算梯度并更新参数</span></span><br><span class=\"line\">        y_hat = net(X)</span><br><span class=\"line\">        l = loss(y_hat, y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class=\"line\">            <span class=\"comment\"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class=\"line\">            updater.zero_grad()</span><br><span class=\"line\">            l.mean().backward()</span><br><span class=\"line\">            updater.step()</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 使用定制的优化器和损失函数</span></span><br><span class=\"line\">            l.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">            updater(X.shape[<span class=\"number\">0</span>])</span><br><span class=\"line\">        metric.add(<span class=\"built_in\">float</span>(l.<span class=\"built_in\">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class=\"line\">    <span class=\"comment\"># 返回训练损失和训练精度</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric[<span class=\"number\">0</span>] / metric[<span class=\"number\">2</span>], metric[<span class=\"number\">1</span>] / metric[<span class=\"number\">2</span>]</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"简洁实现\"><a href=\"#简洁实现\" class=\"headerlink\" title=\"简洁实现\"></a>简洁实现</h2><figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class=\"line\"><span class=\"comment\"># PyTorch不会隐式地调整输入的形状。因此，</span></span><br><span class=\"line\"><span class=\"comment\"># 我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span></span><br><span class=\"line\">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_weights</span>(<span class=\"params\">m</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">type</span>(m) == nn.Linear:</span><br><span class=\"line\">        nn.init.normal_(m.weight, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">net.apply(init_weights);</span><br><span class=\"line\">loss = nn.CrossEntropyLoss(reduction=<span class=\"string\">&#x27;none&#x27;</span>)</span><br><span class=\"line\">trainer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>"},{"layout":"posts","title":"动手学AI-基本知识点","date":"2025-10-14T05:59:06.000Z","_content":"# 模型训练\n## 过拟合和欠拟合\n过拟合就是在训练数据集上训练的“太好”，而不适用于其他的大型真实数据集。\n即如果有足够多的神经元、层数和训练迭代周期， 模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。\n\n欠拟合就是训练误差和验证误差都比较大而且二者之间的差距很小，这就代表模型的表达能力不足，仍需要训练一个更复杂的模型。\n\n## 训练误差和泛化误差\n训练误差（training error）是指， 模型在训练数据集上计算得到的误差。 泛化误差（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。\n\n泛化误差是无法被精确计算出来的，因为无限多数据样本不可能供我们使用，这是一个虚拟的对象，因此只能用一个独立的测试机来估计。\n\n## 模型复杂性\n这是一个比较难定义的东西，可能是参数更多的模型更复杂，也可能是迭代更多次的模型更复杂，也会是训练样本的数量越大越复杂。\n\n## 验证集&测试集\n二者之间的关系非常模糊，因为理论上我们希望测试集能一下测试完模型的误差，这样就能保证独立同分布，但是在实际操作中，因为模型需要不断迭代，每一轮都要用测试集来更新，因此验证集这个概念就被提出，但是他俩还是很难界定，教程里的准确度都用验证集准确度测定。\n\n## K折交叉验证\n把原始训练数据分为K个不重叠的子集，执行K此训练和验证，每次在K-1个子集上进行训练，在剩余的一个子集上验证，最后对K次实验结果取平均来估计误差。\n\n## 权重衰减\n\n","source":"_posts/动手学AI-基本知识点.md","raw":"---\nlayout: posts\ntitle: 动手学AI-基本知识点\ndate: 2025-10-14 13:59:06\ntags: \n - [AI]\n - [机器学习]\ncategories: \n - [学习]\n---\n# 模型训练\n## 过拟合和欠拟合\n过拟合就是在训练数据集上训练的“太好”，而不适用于其他的大型真实数据集。\n即如果有足够多的神经元、层数和训练迭代周期， 模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。\n\n欠拟合就是训练误差和验证误差都比较大而且二者之间的差距很小，这就代表模型的表达能力不足，仍需要训练一个更复杂的模型。\n\n## 训练误差和泛化误差\n训练误差（training error）是指， 模型在训练数据集上计算得到的误差。 泛化误差（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。\n\n泛化误差是无法被精确计算出来的，因为无限多数据样本不可能供我们使用，这是一个虚拟的对象，因此只能用一个独立的测试机来估计。\n\n## 模型复杂性\n这是一个比较难定义的东西，可能是参数更多的模型更复杂，也可能是迭代更多次的模型更复杂，也会是训练样本的数量越大越复杂。\n\n## 验证集&测试集\n二者之间的关系非常模糊，因为理论上我们希望测试集能一下测试完模型的误差，这样就能保证独立同分布，但是在实际操作中，因为模型需要不断迭代，每一轮都要用测试集来更新，因此验证集这个概念就被提出，但是他俩还是很难界定，教程里的准确度都用验证集准确度测定。\n\n## K折交叉验证\n把原始训练数据分为K个不重叠的子集，执行K此训练和验证，每次在K-1个子集上进行训练，在剩余的一个子集上验证，最后对K次实验结果取平均来估计误差。\n\n## 权重衰减\n\n","slug":"动手学AI-基本知识点","published":1,"updated":"2025-10-22T03:13:39.886Z","comments":1,"photos":[],"_id":"cmh4jpy0f0000xoc1aw2t2ji9","content":"<h1 id=\"模型训练\"><a href=\"#模型训练\" class=\"headerlink\" title=\"模型训练\"></a>模型训练</h1><h2 id=\"过拟合和欠拟合\"><a href=\"#过拟合和欠拟合\" class=\"headerlink\" title=\"过拟合和欠拟合\"></a>过拟合和欠拟合</h2><p>过拟合就是在训练数据集上训练的“太好”，而不适用于其他的大型真实数据集。<br>即如果有足够多的神经元、层数和训练迭代周期， 模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。</p>\n<p>欠拟合就是训练误差和验证误差都比较大而且二者之间的差距很小，这就代表模型的表达能力不足，仍需要训练一个更复杂的模型。</p>\n<h2 id=\"训练误差和泛化误差\"><a href=\"#训练误差和泛化误差\" class=\"headerlink\" title=\"训练误差和泛化误差\"></a>训练误差和泛化误差</h2><p>训练误差（training error）是指， 模型在训练数据集上计算得到的误差。 泛化误差（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p>\n<p>泛化误差是无法被精确计算出来的，因为无限多数据样本不可能供我们使用，这是一个虚拟的对象，因此只能用一个独立的测试机来估计。</p>\n<h2 id=\"模型复杂性\"><a href=\"#模型复杂性\" class=\"headerlink\" title=\"模型复杂性\"></a>模型复杂性</h2><p>这是一个比较难定义的东西，可能是参数更多的模型更复杂，也可能是迭代更多次的模型更复杂，也会是训练样本的数量越大越复杂。</p>\n<h2 id=\"验证集-测试集\"><a href=\"#验证集-测试集\" class=\"headerlink\" title=\"验证集&amp;测试集\"></a>验证集&amp;测试集</h2><p>二者之间的关系非常模糊，因为理论上我们希望测试集能一下测试完模型的误差，这样就能保证独立同分布，但是在实际操作中，因为模型需要不断迭代，每一轮都要用测试集来更新，因此验证集这个概念就被提出，但是他俩还是很难界定，教程里的准确度都用验证集准确度测定。</p>\n<h2 id=\"K折交叉验证\"><a href=\"#K折交叉验证\" class=\"headerlink\" title=\"K折交叉验证\"></a>K折交叉验证</h2><p>把原始训练数据分为K个不重叠的子集，执行K此训练和验证，每次在K-1个子集上进行训练，在剩余的一个子集上验证，最后对K次实验结果取平均来估计误差。</p>\n<h2 id=\"权重衰减\"><a href=\"#权重衰减\" class=\"headerlink\" title=\"权重衰减\"></a>权重衰减</h2>","excerpt":"","more":"<h1 id=\"模型训练\"><a href=\"#模型训练\" class=\"headerlink\" title=\"模型训练\"></a>模型训练</h1><h2 id=\"过拟合和欠拟合\"><a href=\"#过拟合和欠拟合\" class=\"headerlink\" title=\"过拟合和欠拟合\"></a>过拟合和欠拟合</h2><p>过拟合就是在训练数据集上训练的“太好”，而不适用于其他的大型真实数据集。<br>即如果有足够多的神经元、层数和训练迭代周期， 模型最终可以在训练集上达到完美的精度，此时测试集的准确性却下降了。</p>\n<p>欠拟合就是训练误差和验证误差都比较大而且二者之间的差距很小，这就代表模型的表达能力不足，仍需要训练一个更复杂的模型。</p>\n<h2 id=\"训练误差和泛化误差\"><a href=\"#训练误差和泛化误差\" class=\"headerlink\" title=\"训练误差和泛化误差\"></a>训练误差和泛化误差</h2><p>训练误差（training error）是指， 模型在训练数据集上计算得到的误差。 泛化误差（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p>\n<p>泛化误差是无法被精确计算出来的，因为无限多数据样本不可能供我们使用，这是一个虚拟的对象，因此只能用一个独立的测试机来估计。</p>\n<h2 id=\"模型复杂性\"><a href=\"#模型复杂性\" class=\"headerlink\" title=\"模型复杂性\"></a>模型复杂性</h2><p>这是一个比较难定义的东西，可能是参数更多的模型更复杂，也可能是迭代更多次的模型更复杂，也会是训练样本的数量越大越复杂。</p>\n<h2 id=\"验证集-测试集\"><a href=\"#验证集-测试集\" class=\"headerlink\" title=\"验证集&amp;测试集\"></a>验证集&amp;测试集</h2><p>二者之间的关系非常模糊，因为理论上我们希望测试集能一下测试完模型的误差，这样就能保证独立同分布，但是在实际操作中，因为模型需要不断迭代，每一轮都要用测试集来更新，因此验证集这个概念就被提出，但是他俩还是很难界定，教程里的准确度都用验证集准确度测定。</p>\n<h2 id=\"K折交叉验证\"><a href=\"#K折交叉验证\" class=\"headerlink\" title=\"K折交叉验证\"></a>K折交叉验证</h2><p>把原始训练数据分为K个不重叠的子集，执行K此训练和验证，每次在K-1个子集上进行训练，在剩余的一个子集上验证，最后对K次实验结果取平均来估计误差。</p>\n<h2 id=\"权重衰减\"><a href=\"#权重衰减\" class=\"headerlink\" title=\"权重衰减\"></a>权重衰减</h2>"},{"layout":"posts","title":"元认知&欺骗大模型","date":"2025-10-20T11:56:07.000Z","_content":"\n# 元认知是什么\n**元认知(Metacognition)** 由美国心理学家*John H. Flavell* 定义，是对一个人的思维过程的认知和对其背后模式的理解。*meta* 这一词根的意思就是“超越”。\n\n其被定义为一个人关于自己的认知过程及结果或其他相关事情的知识，以及为完成某一具体目标或人物，依据认知对象对认知过程进行主动的监测以及连续的调节。通常元认知有两个部分：（1）认知概念（2）认知调节系统。\n\n简而言之，元认知就是**对自己思考过程的认知与理解和调节**，也就是**对认知的认知**\n\n# 论文阅读\n## 论文1：元认知prompting提高大模型理解力（NAACL-HLT）\n### 研究背景\n在LLM里，受有效提示设计的影响，任务的特定性表现一直在进步。近期的提示词工作已经增强了LLM的逻辑密集型任务推理能力，但是对细微差异的理解尤其是对处理和解释复杂信息至关重要的信息，还没有得到充分研究。在此背景下，作者引入了元认知提示Metacognitive Prompting（MP），并且在llama，PaLM2，GPT-3.5和GPT-4上做了实验，在GLUE、SuperGLUE、BLUE和LexGLUE基准测试的十个自然语言理解（NLU）数据集上进行测试。\n\n简单增加模型规模不一定能增加理解和推理能力。深入研究提示词带来的收益媲美微调，且增加样本效率。\n\n### 从人到LLM\n![](../images/human-llm-metacogiitive.png)\n* LLM解释提供的文本，即人类的理解阶段\n* 模型进行初步判断解释，即人类基于信息产生判断的阶段\n* 模型进行自我反思，进行批判性评估\n* 反思之后进行决策，阐述其推理依据\n* 评估可信性（信心）\n\n![](../images/metacognitive-prompt.png)\n\n### 总结\n本篇论文主要就是讲了怎么通过认知方法来提高大模型的理解能力，主要工作是给LLM配置一个模拟自我反思认知的过程。\n本文样例：\n> 例1：Zero-shot MP Template\n\n* In two sentences, sentence 1: “[sentence_1]” and sentence 2: “[sentence_2]”, both contain the target word “[word]”. Determine if the target word is used with the same meaning in both sentences. As you perform this task, follow these steps:\n1. Understand the context and meaning of the target word in both sentences.\n2. Make a preliminary judgment on whether the target word has the same meaning in both sentences.\n3. Critically assess your preliminary analysis. If you are unsure about your initial judgment, reassess it.\n4. Confirm your final answer and explain the reasoning behind your decision.\n5. Evaluate your confidence (0-100%) in your analysis and provide an explanation for this confidence level.\n\n> 例2：Few-shot MP Demonstration\n* In two sentences, sentence 1: “The washing machine won’t go unless it’s plugged in.” and sentence 2: “The day went well until I got your call.”, both contain the target word “go”. Determine if the target word is used with the same meaning in both sentences.\n1. In sentence 1, “go” refers to the operation of the washing machine. In sentence 2, “went” (a form of “go”) refers to the progression of the day.  \n2. Initially, “go” in the first sentence seems to be about mechanical operation, while in the second, it’s about the passage of time or events.  \n3. Reflecting further, the contexts clearly differentiate the meanings: one deals with machinery and its function, the other deals with the experience of a day. \n4. The target word “go” does not have the same meaning in both sentences. In the first, it pertains to operation, and in the second, to the progression of events.  \n5. I am 83% confident in this analysis. The term “go” is used in different contexts in the two sentences, leading to different meanings.\n\n## 论文2：生成式人工智能的元认知需求与机遇（ACM CHI‘24）\n### 研究背景\n生成式人工智能在提示词、评估和以来输出以及优化工作流程方面面临挑战。作者认为元认知监控一个人的思想和行为的心理能力，能为理解和设计这些可用性提供有价值的帮助，并且最近的生成式人工智能用户研究里说明了生成式AI如何对用户施加元认知需求，需要高度的元认知监控和控制。\n\n作者提出，提供把元认知支持策略集成到生成式人工智能系统中来针对可解释性和可定制性来满足这些需求，并说明这是推进人机交互进步的新颖的研究和设计方向。\n\n> 原文：we suggest that current GenAI systems impose multiple metacognitive demands on users; understanding these demands can help interpret and probe the identified and potentially novel usability challenges. Secondly, we suggest that the perspective of metacognitive demands offers new research and design opportunities for human-AI interaction. \n\nLLM对用户有着元认知需求。这种需求就像经理把任务委派给团队，经理需要清楚的理解并且制定团队的目标，把目标分解为可沟通的任务，评估团队的产出质量并且在此过程中相应的调整计划；此外他们需要决定是否、何时以及如何委派这些任务，这些能力涉及对一个人的思维过程和行为的元认知监控和控制。\n\n### 做了什么\n通过可以集成到GenAI系统中的元认知支持策略来提高用户的元认知，包括帮助用户进行规划、自我评估和自我管理的策略，通过一定的设计把元认知处理从用户转到系统，即降低对用户的元认知能力要求。\n\n本文贡献：\n*  We conceptualize and ground the usability challenges of GenAI in an understanding of human metacognition, drawing on research from psychological and cognitive science and recent GenAI user studies. \n*  We draw from research on metacognitive interventions, GenAI prototypes, and human-AI interaction to propose two directions for addressing the metacognitive demands of GenAI: improving users’ metacognition, and reducing the metacognitive demands of GenAI. \n*  We use the metacognition lens to identify the need—and concrete directions—for further research into the metacognitive demands of GenAI, and design opportunities that leverage the unique properties of GenAI to augment system usability.\n*  定义好了概念、提出了解决 GenAI 元认知需求的两个方向：提高用户的元认知，以及降低 GenAI 的元认知需求、进一步研究 GenAI 元认知需求和具体方向，并设计利用 GenAI 的独特属性来增强系统可用性（说实话没有很理解什么意思）\n  \n### 本文提出的元认知框架：\n![](../images/MetaFrame.png)\n分为元认知知识和元认知经验，这是理解自己认知的两种不同信息来源，以及监控和控制的元认知能力，通过这种能力，人们可以评估和指导自己的认知。\n\n**元认知知识**是明确的，包括人们对自己的策略、推理能力、决策和信念等方面的有意识理解。\n**元认知经验**包括人们可以直接感受的任何事情，并且可以是隐式的，包括主观感受，例如熟悉的感觉，或在阅读时误解了段落的感觉，以及提供有关认知处理信息的其他隐含线索。\n\n知识和经验二者是互相关联的，经验有助于认知知识（例如，当解决问题过程中的困难感被编码为一个人不擅长解决问题的知识时）。知识也是一种元认知经验（在经历一种困难的感觉时记得自己解决问题的能力很差）\n\n**元认知监控**是对自己思维的评估，而 **元认知控制** 则是直接指导自己的思维能力。二者也是互相关联的。\n\n### 生成式人工智能在简化的用户工作流程中的每个点提出的元认知需求\n![](../images/genai.png)\n\n### 总结\n这篇主要是做人机交互方面的研究，目的是尽量降低LLM对用户的元认知要求，主要目标没有什么参考性，只需要关注他们对LLM的元认知理解。\n\n## 论文3：通过有说服力的对话调查LLM对错误信息的信念\n### 背景\n在多轮对话中，尤其是有说服力的对话中，LLM的信念可能会发生变化，即LLM可以高度接受外部证据，即使与它们的记忆冲突。而且LLM更倾向于调整自己的答案，甚至遵循客观上错误的观点。\n之前的工作主要集中在单论对话中，但是一个人的信念是可以通过说服来改变的，说服过程当然可以包含多轮对话。\n### ","source":"_posts/调研-论文阅读.md","raw":"---\nlayout: posts\ntitle: 元认知&欺骗大模型\ndate: 2025-10-20 19:56:07\ntags: \n -[论文阅读]\n -[调研]\ncategories: \n - [科研]\n---\n\n# 元认知是什么\n**元认知(Metacognition)** 由美国心理学家*John H. Flavell* 定义，是对一个人的思维过程的认知和对其背后模式的理解。*meta* 这一词根的意思就是“超越”。\n\n其被定义为一个人关于自己的认知过程及结果或其他相关事情的知识，以及为完成某一具体目标或人物，依据认知对象对认知过程进行主动的监测以及连续的调节。通常元认知有两个部分：（1）认知概念（2）认知调节系统。\n\n简而言之，元认知就是**对自己思考过程的认知与理解和调节**，也就是**对认知的认知**\n\n# 论文阅读\n## 论文1：元认知prompting提高大模型理解力（NAACL-HLT）\n### 研究背景\n在LLM里，受有效提示设计的影响，任务的特定性表现一直在进步。近期的提示词工作已经增强了LLM的逻辑密集型任务推理能力，但是对细微差异的理解尤其是对处理和解释复杂信息至关重要的信息，还没有得到充分研究。在此背景下，作者引入了元认知提示Metacognitive Prompting（MP），并且在llama，PaLM2，GPT-3.5和GPT-4上做了实验，在GLUE、SuperGLUE、BLUE和LexGLUE基准测试的十个自然语言理解（NLU）数据集上进行测试。\n\n简单增加模型规模不一定能增加理解和推理能力。深入研究提示词带来的收益媲美微调，且增加样本效率。\n\n### 从人到LLM\n![](../images/human-llm-metacogiitive.png)\n* LLM解释提供的文本，即人类的理解阶段\n* 模型进行初步判断解释，即人类基于信息产生判断的阶段\n* 模型进行自我反思，进行批判性评估\n* 反思之后进行决策，阐述其推理依据\n* 评估可信性（信心）\n\n![](../images/metacognitive-prompt.png)\n\n### 总结\n本篇论文主要就是讲了怎么通过认知方法来提高大模型的理解能力，主要工作是给LLM配置一个模拟自我反思认知的过程。\n本文样例：\n> 例1：Zero-shot MP Template\n\n* In two sentences, sentence 1: “[sentence_1]” and sentence 2: “[sentence_2]”, both contain the target word “[word]”. Determine if the target word is used with the same meaning in both sentences. As you perform this task, follow these steps:\n1. Understand the context and meaning of the target word in both sentences.\n2. Make a preliminary judgment on whether the target word has the same meaning in both sentences.\n3. Critically assess your preliminary analysis. If you are unsure about your initial judgment, reassess it.\n4. Confirm your final answer and explain the reasoning behind your decision.\n5. Evaluate your confidence (0-100%) in your analysis and provide an explanation for this confidence level.\n\n> 例2：Few-shot MP Demonstration\n* In two sentences, sentence 1: “The washing machine won’t go unless it’s plugged in.” and sentence 2: “The day went well until I got your call.”, both contain the target word “go”. Determine if the target word is used with the same meaning in both sentences.\n1. In sentence 1, “go” refers to the operation of the washing machine. In sentence 2, “went” (a form of “go”) refers to the progression of the day.  \n2. Initially, “go” in the first sentence seems to be about mechanical operation, while in the second, it’s about the passage of time or events.  \n3. Reflecting further, the contexts clearly differentiate the meanings: one deals with machinery and its function, the other deals with the experience of a day. \n4. The target word “go” does not have the same meaning in both sentences. In the first, it pertains to operation, and in the second, to the progression of events.  \n5. I am 83% confident in this analysis. The term “go” is used in different contexts in the two sentences, leading to different meanings.\n\n## 论文2：生成式人工智能的元认知需求与机遇（ACM CHI‘24）\n### 研究背景\n生成式人工智能在提示词、评估和以来输出以及优化工作流程方面面临挑战。作者认为元认知监控一个人的思想和行为的心理能力，能为理解和设计这些可用性提供有价值的帮助，并且最近的生成式人工智能用户研究里说明了生成式AI如何对用户施加元认知需求，需要高度的元认知监控和控制。\n\n作者提出，提供把元认知支持策略集成到生成式人工智能系统中来针对可解释性和可定制性来满足这些需求，并说明这是推进人机交互进步的新颖的研究和设计方向。\n\n> 原文：we suggest that current GenAI systems impose multiple metacognitive demands on users; understanding these demands can help interpret and probe the identified and potentially novel usability challenges. Secondly, we suggest that the perspective of metacognitive demands offers new research and design opportunities for human-AI interaction. \n\nLLM对用户有着元认知需求。这种需求就像经理把任务委派给团队，经理需要清楚的理解并且制定团队的目标，把目标分解为可沟通的任务，评估团队的产出质量并且在此过程中相应的调整计划；此外他们需要决定是否、何时以及如何委派这些任务，这些能力涉及对一个人的思维过程和行为的元认知监控和控制。\n\n### 做了什么\n通过可以集成到GenAI系统中的元认知支持策略来提高用户的元认知，包括帮助用户进行规划、自我评估和自我管理的策略，通过一定的设计把元认知处理从用户转到系统，即降低对用户的元认知能力要求。\n\n本文贡献：\n*  We conceptualize and ground the usability challenges of GenAI in an understanding of human metacognition, drawing on research from psychological and cognitive science and recent GenAI user studies. \n*  We draw from research on metacognitive interventions, GenAI prototypes, and human-AI interaction to propose two directions for addressing the metacognitive demands of GenAI: improving users’ metacognition, and reducing the metacognitive demands of GenAI. \n*  We use the metacognition lens to identify the need—and concrete directions—for further research into the metacognitive demands of GenAI, and design opportunities that leverage the unique properties of GenAI to augment system usability.\n*  定义好了概念、提出了解决 GenAI 元认知需求的两个方向：提高用户的元认知，以及降低 GenAI 的元认知需求、进一步研究 GenAI 元认知需求和具体方向，并设计利用 GenAI 的独特属性来增强系统可用性（说实话没有很理解什么意思）\n  \n### 本文提出的元认知框架：\n![](../images/MetaFrame.png)\n分为元认知知识和元认知经验，这是理解自己认知的两种不同信息来源，以及监控和控制的元认知能力，通过这种能力，人们可以评估和指导自己的认知。\n\n**元认知知识**是明确的，包括人们对自己的策略、推理能力、决策和信念等方面的有意识理解。\n**元认知经验**包括人们可以直接感受的任何事情，并且可以是隐式的，包括主观感受，例如熟悉的感觉，或在阅读时误解了段落的感觉，以及提供有关认知处理信息的其他隐含线索。\n\n知识和经验二者是互相关联的，经验有助于认知知识（例如，当解决问题过程中的困难感被编码为一个人不擅长解决问题的知识时）。知识也是一种元认知经验（在经历一种困难的感觉时记得自己解决问题的能力很差）\n\n**元认知监控**是对自己思维的评估，而 **元认知控制** 则是直接指导自己的思维能力。二者也是互相关联的。\n\n### 生成式人工智能在简化的用户工作流程中的每个点提出的元认知需求\n![](../images/genai.png)\n\n### 总结\n这篇主要是做人机交互方面的研究，目的是尽量降低LLM对用户的元认知要求，主要目标没有什么参考性，只需要关注他们对LLM的元认知理解。\n\n## 论文3：通过有说服力的对话调查LLM对错误信息的信念\n### 背景\n在多轮对话中，尤其是有说服力的对话中，LLM的信念可能会发生变化，即LLM可以高度接受外部证据，即使与它们的记忆冲突。而且LLM更倾向于调整自己的答案，甚至遵循客观上错误的观点。\n之前的工作主要集中在单论对话中，但是一个人的信念是可以通过说服来改变的，说服过程当然可以包含多轮对话。\n### ","slug":"调研-论文阅读","published":1,"updated":"2025-10-24T09:06:04.034Z","_id":"cmh4jpy0i0002xoc18kq6dcrj","comments":1,"photos":[],"content":"<h1 id=\"元认知是什么\"><a href=\"#元认知是什么\" class=\"headerlink\" title=\"元认知是什么\"></a>元认知是什么</h1><p><strong>元认知(Metacognition)</strong> 由美国心理学家<em>John H. Flavell</em> 定义，是对一个人的思维过程的认知和对其背后模式的理解。<em>meta</em> 这一词根的意思就是“超越”。</p>\n<p>其被定义为一个人关于自己的认知过程及结果或其他相关事情的知识，以及为完成某一具体目标或人物，依据认知对象对认知过程进行主动的监测以及连续的调节。通常元认知有两个部分：（1）认知概念（2）认知调节系统。</p>\n<p>简而言之，元认知就是<strong>对自己思考过程的认知与理解和调节</strong>，也就是<strong>对认知的认知</strong></p>\n<h1 id=\"论文阅读\"><a href=\"#论文阅读\" class=\"headerlink\" title=\"论文阅读\"></a>论文阅读</h1><h2 id=\"论文1：元认知prompting提高大模型理解力（NAACL-HLT）\"><a href=\"#论文1：元认知prompting提高大模型理解力（NAACL-HLT）\" class=\"headerlink\" title=\"论文1：元认知prompting提高大模型理解力（NAACL-HLT）\"></a>论文1：元认知prompting提高大模型理解力（NAACL-HLT）</h2><h3 id=\"研究背景\"><a href=\"#研究背景\" class=\"headerlink\" title=\"研究背景\"></a>研究背景</h3><p>在LLM里，受有效提示设计的影响，任务的特定性表现一直在进步。近期的提示词工作已经增强了LLM的逻辑密集型任务推理能力，但是对细微差异的理解尤其是对处理和解释复杂信息至关重要的信息，还没有得到充分研究。在此背景下，作者引入了元认知提示Metacognitive Prompting（MP），并且在llama，PaLM2，GPT-3.5和GPT-4上做了实验，在GLUE、SuperGLUE、BLUE和LexGLUE基准测试的十个自然语言理解（NLU）数据集上进行测试。</p>\n<p>简单增加模型规模不一定能增加理解和推理能力。深入研究提示词带来的收益媲美微调，且增加样本效率。</p>\n<h3 id=\"从人到LLM\"><a href=\"#从人到LLM\" class=\"headerlink\" title=\"从人到LLM\"></a>从人到LLM</h3><p><img src=\"/../images/human-llm-metacogiitive.png\"></p>\n<ul>\n<li>LLM解释提供的文本，即人类的理解阶段</li>\n<li>模型进行初步判断解释，即人类基于信息产生判断的阶段</li>\n<li>模型进行自我反思，进行批判性评估</li>\n<li>反思之后进行决策，阐述其推理依据</li>\n<li>评估可信性（信心）</li>\n</ul>\n<p><img src=\"/../images/metacognitive-prompt.png\"></p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本篇论文主要就是讲了怎么通过认知方法来提高大模型的理解能力，主要工作是给LLM配置一个模拟自我反思认知的过程。<br>本文样例：</p>\n<blockquote>\n<p>例1：Zero-shot MP Template</p>\n</blockquote>\n<ul>\n<li>In two sentences, sentence 1: “[sentence_1]” and sentence 2: “[sentence_2]”, both contain the target word “[word]”. Determine if the target word is used with the same meaning in both sentences. As you perform this task, follow these steps:</li>\n</ul>\n<ol>\n<li>Understand the context and meaning of the target word in both sentences.</li>\n<li>Make a preliminary judgment on whether the target word has the same meaning in both sentences.</li>\n<li>Critically assess your preliminary analysis. If you are unsure about your initial judgment, reassess it.</li>\n<li>Confirm your final answer and explain the reasoning behind your decision.</li>\n<li>Evaluate your confidence (0-100%) in your analysis and provide an explanation for this confidence level.</li>\n</ol>\n<blockquote>\n<p>例2：Few-shot MP Demonstration</p>\n</blockquote>\n<ul>\n<li>In two sentences, sentence 1: “The washing machine won’t go unless it’s plugged in.” and sentence 2: “The day went well until I got your call.”, both contain the target word “go”. Determine if the target word is used with the same meaning in both sentences.</li>\n</ul>\n<ol>\n<li>In sentence 1, “go” refers to the operation of the washing machine. In sentence 2, “went” (a form of “go”) refers to the progression of the day.  </li>\n<li>Initially, “go” in the first sentence seems to be about mechanical operation, while in the second, it’s about the passage of time or events.  </li>\n<li>Reflecting further, the contexts clearly differentiate the meanings: one deals with machinery and its function, the other deals with the experience of a day. </li>\n<li>The target word “go” does not have the same meaning in both sentences. In the first, it pertains to operation, and in the second, to the progression of events.  </li>\n<li>I am 83% confident in this analysis. The term “go” is used in different contexts in the two sentences, leading to different meanings.</li>\n</ol>\n<h2 id=\"论文2：生成式人工智能的元认知需求与机遇（ACM-CHI‘24）\"><a href=\"#论文2：生成式人工智能的元认知需求与机遇（ACM-CHI‘24）\" class=\"headerlink\" title=\"论文2：生成式人工智能的元认知需求与机遇（ACM CHI‘24）\"></a>论文2：生成式人工智能的元认知需求与机遇（ACM CHI‘24）</h2><h3 id=\"研究背景-1\"><a href=\"#研究背景-1\" class=\"headerlink\" title=\"研究背景\"></a>研究背景</h3><p>生成式人工智能在提示词、评估和以来输出以及优化工作流程方面面临挑战。作者认为元认知监控一个人的思想和行为的心理能力，能为理解和设计这些可用性提供有价值的帮助，并且最近的生成式人工智能用户研究里说明了生成式AI如何对用户施加元认知需求，需要高度的元认知监控和控制。</p>\n<p>作者提出，提供把元认知支持策略集成到生成式人工智能系统中来针对可解释性和可定制性来满足这些需求，并说明这是推进人机交互进步的新颖的研究和设计方向。</p>\n<blockquote>\n<p>原文：we suggest that current GenAI systems impose multiple metacognitive demands on users; understanding these demands can help interpret and probe the identified and potentially novel usability challenges. Secondly, we suggest that the perspective of metacognitive demands offers new research and design opportunities for human-AI interaction. </p>\n</blockquote>\n<p>LLM对用户有着元认知需求。这种需求就像经理把任务委派给团队，经理需要清楚的理解并且制定团队的目标，把目标分解为可沟通的任务，评估团队的产出质量并且在此过程中相应的调整计划；此外他们需要决定是否、何时以及如何委派这些任务，这些能力涉及对一个人的思维过程和行为的元认知监控和控制。</p>\n<h3 id=\"做了什么\"><a href=\"#做了什么\" class=\"headerlink\" title=\"做了什么\"></a>做了什么</h3><p>通过可以集成到GenAI系统中的元认知支持策略来提高用户的元认知，包括帮助用户进行规划、自我评估和自我管理的策略，通过一定的设计把元认知处理从用户转到系统，即降低对用户的元认知能力要求。</p>\n<p>本文贡献：</p>\n<ul>\n<li>We conceptualize and ground the usability challenges of GenAI in an understanding of human metacognition, drawing on research from psychological and cognitive science and recent GenAI user studies. </li>\n<li>We draw from research on metacognitive interventions, GenAI prototypes, and human-AI interaction to propose two directions for addressing the metacognitive demands of GenAI: improving users’ metacognition, and reducing the metacognitive demands of GenAI. </li>\n<li>We use the metacognition lens to identify the need—and concrete directions—for further research into the metacognitive demands of GenAI, and design opportunities that leverage the unique properties of GenAI to augment system usability.</li>\n<li>定义好了概念、提出了解决 GenAI 元认知需求的两个方向：提高用户的元认知，以及降低 GenAI 的元认知需求、进一步研究 GenAI 元认知需求和具体方向，并设计利用 GenAI 的独特属性来增强系统可用性（说实话没有很理解什么意思）</li>\n</ul>\n<h3 id=\"本文提出的元认知框架：\"><a href=\"#本文提出的元认知框架：\" class=\"headerlink\" title=\"本文提出的元认知框架：\"></a>本文提出的元认知框架：</h3><p><img src=\"/../images/MetaFrame.png\"><br>分为元认知知识和元认知经验，这是理解自己认知的两种不同信息来源，以及监控和控制的元认知能力，通过这种能力，人们可以评估和指导自己的认知。</p>\n<p><strong>元认知知识</strong>是明确的，包括人们对自己的策略、推理能力、决策和信念等方面的有意识理解。<br><strong>元认知经验</strong>包括人们可以直接感受的任何事情，并且可以是隐式的，包括主观感受，例如熟悉的感觉，或在阅读时误解了段落的感觉，以及提供有关认知处理信息的其他隐含线索。</p>\n<p>知识和经验二者是互相关联的，经验有助于认知知识（例如，当解决问题过程中的困难感被编码为一个人不擅长解决问题的知识时）。知识也是一种元认知经验（在经历一种困难的感觉时记得自己解决问题的能力很差）</p>\n<p><strong>元认知监控</strong>是对自己思维的评估，而 <strong>元认知控制</strong> 则是直接指导自己的思维能力。二者也是互相关联的。</p>\n<h3 id=\"生成式人工智能在简化的用户工作流程中的每个点提出的元认知需求\"><a href=\"#生成式人工智能在简化的用户工作流程中的每个点提出的元认知需求\" class=\"headerlink\" title=\"生成式人工智能在简化的用户工作流程中的每个点提出的元认知需求\"></a>生成式人工智能在简化的用户工作流程中的每个点提出的元认知需求</h3><p><img src=\"/../images/genai.png\"></p>\n<h3 id=\"总结-1\"><a href=\"#总结-1\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>这篇主要是做人机交互方面的研究，目的是尽量降低LLM对用户的元认知要求，主要目标没有什么参考性，只需要关注他们对LLM的元认知理解。</p>\n<h2 id=\"论文3：通过有说服力的对话调查LLM对错误信息的信念\"><a href=\"#论文3：通过有说服力的对话调查LLM对错误信息的信念\" class=\"headerlink\" title=\"论文3：通过有说服力的对话调查LLM对错误信息的信念\"></a>论文3：通过有说服力的对话调查LLM对错误信息的信念</h2><h3 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h3><p>在多轮对话中，尤其是有说服力的对话中，LLM的信念可能会发生变化，即LLM可以高度接受外部证据，即使与它们的记忆冲突。而且LLM更倾向于调整自己的答案，甚至遵循客观上错误的观点。<br>之前的工作主要集中在单论对话中，但是一个人的信念是可以通过说服来改变的，说服过程当然可以包含多轮对话。</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3>","excerpt":"","more":"<h1 id=\"元认知是什么\"><a href=\"#元认知是什么\" class=\"headerlink\" title=\"元认知是什么\"></a>元认知是什么</h1><p><strong>元认知(Metacognition)</strong> 由美国心理学家<em>John H. Flavell</em> 定义，是对一个人的思维过程的认知和对其背后模式的理解。<em>meta</em> 这一词根的意思就是“超越”。</p>\n<p>其被定义为一个人关于自己的认知过程及结果或其他相关事情的知识，以及为完成某一具体目标或人物，依据认知对象对认知过程进行主动的监测以及连续的调节。通常元认知有两个部分：（1）认知概念（2）认知调节系统。</p>\n<p>简而言之，元认知就是<strong>对自己思考过程的认知与理解和调节</strong>，也就是<strong>对认知的认知</strong></p>\n<h1 id=\"论文阅读\"><a href=\"#论文阅读\" class=\"headerlink\" title=\"论文阅读\"></a>论文阅读</h1><h2 id=\"论文1：元认知prompting提高大模型理解力（NAACL-HLT）\"><a href=\"#论文1：元认知prompting提高大模型理解力（NAACL-HLT）\" class=\"headerlink\" title=\"论文1：元认知prompting提高大模型理解力（NAACL-HLT）\"></a>论文1：元认知prompting提高大模型理解力（NAACL-HLT）</h2><h3 id=\"研究背景\"><a href=\"#研究背景\" class=\"headerlink\" title=\"研究背景\"></a>研究背景</h3><p>在LLM里，受有效提示设计的影响，任务的特定性表现一直在进步。近期的提示词工作已经增强了LLM的逻辑密集型任务推理能力，但是对细微差异的理解尤其是对处理和解释复杂信息至关重要的信息，还没有得到充分研究。在此背景下，作者引入了元认知提示Metacognitive Prompting（MP），并且在llama，PaLM2，GPT-3.5和GPT-4上做了实验，在GLUE、SuperGLUE、BLUE和LexGLUE基准测试的十个自然语言理解（NLU）数据集上进行测试。</p>\n<p>简单增加模型规模不一定能增加理解和推理能力。深入研究提示词带来的收益媲美微调，且增加样本效率。</p>\n<h3 id=\"从人到LLM\"><a href=\"#从人到LLM\" class=\"headerlink\" title=\"从人到LLM\"></a>从人到LLM</h3><p><img src=\"/../images/human-llm-metacogiitive.png\"></p>\n<ul>\n<li>LLM解释提供的文本，即人类的理解阶段</li>\n<li>模型进行初步判断解释，即人类基于信息产生判断的阶段</li>\n<li>模型进行自我反思，进行批判性评估</li>\n<li>反思之后进行决策，阐述其推理依据</li>\n<li>评估可信性（信心）</li>\n</ul>\n<p><img src=\"/../images/metacognitive-prompt.png\"></p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本篇论文主要就是讲了怎么通过认知方法来提高大模型的理解能力，主要工作是给LLM配置一个模拟自我反思认知的过程。<br>本文样例：</p>\n<blockquote>\n<p>例1：Zero-shot MP Template</p>\n</blockquote>\n<ul>\n<li>In two sentences, sentence 1: “[sentence_1]” and sentence 2: “[sentence_2]”, both contain the target word “[word]”. Determine if the target word is used with the same meaning in both sentences. As you perform this task, follow these steps:</li>\n</ul>\n<ol>\n<li>Understand the context and meaning of the target word in both sentences.</li>\n<li>Make a preliminary judgment on whether the target word has the same meaning in both sentences.</li>\n<li>Critically assess your preliminary analysis. If you are unsure about your initial judgment, reassess it.</li>\n<li>Confirm your final answer and explain the reasoning behind your decision.</li>\n<li>Evaluate your confidence (0-100%) in your analysis and provide an explanation for this confidence level.</li>\n</ol>\n<blockquote>\n<p>例2：Few-shot MP Demonstration</p>\n</blockquote>\n<ul>\n<li>In two sentences, sentence 1: “The washing machine won’t go unless it’s plugged in.” and sentence 2: “The day went well until I got your call.”, both contain the target word “go”. Determine if the target word is used with the same meaning in both sentences.</li>\n</ul>\n<ol>\n<li>In sentence 1, “go” refers to the operation of the washing machine. In sentence 2, “went” (a form of “go”) refers to the progression of the day.  </li>\n<li>Initially, “go” in the first sentence seems to be about mechanical operation, while in the second, it’s about the passage of time or events.  </li>\n<li>Reflecting further, the contexts clearly differentiate the meanings: one deals with machinery and its function, the other deals with the experience of a day. </li>\n<li>The target word “go” does not have the same meaning in both sentences. In the first, it pertains to operation, and in the second, to the progression of events.  </li>\n<li>I am 83% confident in this analysis. The term “go” is used in different contexts in the two sentences, leading to different meanings.</li>\n</ol>\n<h2 id=\"论文2：生成式人工智能的元认知需求与机遇（ACM-CHI‘24）\"><a href=\"#论文2：生成式人工智能的元认知需求与机遇（ACM-CHI‘24）\" class=\"headerlink\" title=\"论文2：生成式人工智能的元认知需求与机遇（ACM CHI‘24）\"></a>论文2：生成式人工智能的元认知需求与机遇（ACM CHI‘24）</h2><h3 id=\"研究背景-1\"><a href=\"#研究背景-1\" class=\"headerlink\" title=\"研究背景\"></a>研究背景</h3><p>生成式人工智能在提示词、评估和以来输出以及优化工作流程方面面临挑战。作者认为元认知监控一个人的思想和行为的心理能力，能为理解和设计这些可用性提供有价值的帮助，并且最近的生成式人工智能用户研究里说明了生成式AI如何对用户施加元认知需求，需要高度的元认知监控和控制。</p>\n<p>作者提出，提供把元认知支持策略集成到生成式人工智能系统中来针对可解释性和可定制性来满足这些需求，并说明这是推进人机交互进步的新颖的研究和设计方向。</p>\n<blockquote>\n<p>原文：we suggest that current GenAI systems impose multiple metacognitive demands on users; understanding these demands can help interpret and probe the identified and potentially novel usability challenges. Secondly, we suggest that the perspective of metacognitive demands offers new research and design opportunities for human-AI interaction. </p>\n</blockquote>\n<p>LLM对用户有着元认知需求。这种需求就像经理把任务委派给团队，经理需要清楚的理解并且制定团队的目标，把目标分解为可沟通的任务，评估团队的产出质量并且在此过程中相应的调整计划；此外他们需要决定是否、何时以及如何委派这些任务，这些能力涉及对一个人的思维过程和行为的元认知监控和控制。</p>\n<h3 id=\"做了什么\"><a href=\"#做了什么\" class=\"headerlink\" title=\"做了什么\"></a>做了什么</h3><p>通过可以集成到GenAI系统中的元认知支持策略来提高用户的元认知，包括帮助用户进行规划、自我评估和自我管理的策略，通过一定的设计把元认知处理从用户转到系统，即降低对用户的元认知能力要求。</p>\n<p>本文贡献：</p>\n<ul>\n<li>We conceptualize and ground the usability challenges of GenAI in an understanding of human metacognition, drawing on research from psychological and cognitive science and recent GenAI user studies. </li>\n<li>We draw from research on metacognitive interventions, GenAI prototypes, and human-AI interaction to propose two directions for addressing the metacognitive demands of GenAI: improving users’ metacognition, and reducing the metacognitive demands of GenAI. </li>\n<li>We use the metacognition lens to identify the need—and concrete directions—for further research into the metacognitive demands of GenAI, and design opportunities that leverage the unique properties of GenAI to augment system usability.</li>\n<li>定义好了概念、提出了解决 GenAI 元认知需求的两个方向：提高用户的元认知，以及降低 GenAI 的元认知需求、进一步研究 GenAI 元认知需求和具体方向，并设计利用 GenAI 的独特属性来增强系统可用性（说实话没有很理解什么意思）</li>\n</ul>\n<h3 id=\"本文提出的元认知框架：\"><a href=\"#本文提出的元认知框架：\" class=\"headerlink\" title=\"本文提出的元认知框架：\"></a>本文提出的元认知框架：</h3><p><img src=\"/../images/MetaFrame.png\"><br>分为元认知知识和元认知经验，这是理解自己认知的两种不同信息来源，以及监控和控制的元认知能力，通过这种能力，人们可以评估和指导自己的认知。</p>\n<p><strong>元认知知识</strong>是明确的，包括人们对自己的策略、推理能力、决策和信念等方面的有意识理解。<br><strong>元认知经验</strong>包括人们可以直接感受的任何事情，并且可以是隐式的，包括主观感受，例如熟悉的感觉，或在阅读时误解了段落的感觉，以及提供有关认知处理信息的其他隐含线索。</p>\n<p>知识和经验二者是互相关联的，经验有助于认知知识（例如，当解决问题过程中的困难感被编码为一个人不擅长解决问题的知识时）。知识也是一种元认知经验（在经历一种困难的感觉时记得自己解决问题的能力很差）</p>\n<p><strong>元认知监控</strong>是对自己思维的评估，而 <strong>元认知控制</strong> 则是直接指导自己的思维能力。二者也是互相关联的。</p>\n<h3 id=\"生成式人工智能在简化的用户工作流程中的每个点提出的元认知需求\"><a href=\"#生成式人工智能在简化的用户工作流程中的每个点提出的元认知需求\" class=\"headerlink\" title=\"生成式人工智能在简化的用户工作流程中的每个点提出的元认知需求\"></a>生成式人工智能在简化的用户工作流程中的每个点提出的元认知需求</h3><p><img src=\"/../images/genai.png\"></p>\n<h3 id=\"总结-1\"><a href=\"#总结-1\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>这篇主要是做人机交互方面的研究，目的是尽量降低LLM对用户的元认知要求，主要目标没有什么参考性，只需要关注他们对LLM的元认知理解。</p>\n<h2 id=\"论文3：通过有说服力的对话调查LLM对错误信息的信念\"><a href=\"#论文3：通过有说服力的对话调查LLM对错误信息的信念\" class=\"headerlink\" title=\"论文3：通过有说服力的对话调查LLM对错误信息的信念\"></a>论文3：通过有说服力的对话调查LLM对错误信息的信念</h2><h3 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h3><p>在多轮对话中，尤其是有说服力的对话中，LLM的信念可能会发生变化，即LLM可以高度接受外部证据，即使与它们的记忆冲突。而且LLM更倾向于调整自己的答案，甚至遵循客观上错误的观点。<br>之前的工作主要集中在单论对话中，但是一个人的信念是可以通过说服来改变的，说服过程当然可以包含多轮对话。</p>\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3>"}],"PostAsset":[],"PostCategory":[{"post_id":"cmgnsr37j0001f4gfbeo357xb","category_id":"cmgnsr37o0006f4gf7h935gwx","_id":"cmgnsr37q000bf4gf6d661kcb"},{"post_id":"cmgnsr37m0003f4gf0tij76ty","category_id":"cmgnsr37o0006f4gf7h935gwx","_id":"cmgnsr37r000ff4gfeac0dq6x"},{"post_id":"cmh4jpy0f0000xoc1aw2t2ji9","category_id":"cmgnsr37o0006f4gf7h935gwx","_id":"cmh4jpy0l0006xoc1gvwybmlz"},{"post_id":"cmh4jpy0i0002xoc18kq6dcrj","category_id":"cmh4jpy0k0005xoc1gq8jbz8y","_id":"cmh4jpy0l0008xoc14nea7uen"}],"PostTag":[{"post_id":"cmgnsr37j0001f4gfbeo357xb","tag_id":"cmgnsr37n0004f4gfdnux759b","_id":"cmgnsr37q000cf4gf1fe39noo"},{"post_id":"cmgnsr37j0001f4gfbeo357xb","tag_id":"cmgnsr37p0008f4gf83cta7v2","_id":"cmgnsr37q000df4gfhmeu6oza"},{"post_id":"cmgnsr37m0003f4gf0tij76ty","tag_id":"cmgnsr37n0004f4gfdnux759b","_id":"cmgnsr37r000gf4gfd9g54pq0"},{"post_id":"cmgnsr37m0003f4gf0tij76ty","tag_id":"cmgnsr37p0008f4gf83cta7v2","_id":"cmgnsr37r000hf4gf002q3zyt"},{"post_id":"cmh4jpy0f0000xoc1aw2t2ji9","tag_id":"cmgnsr37n0004f4gfdnux759b","_id":"cmh4jpy0i0001xoc18ol393dj"},{"post_id":"cmh4jpy0f0000xoc1aw2t2ji9","tag_id":"cmgnsr37p0008f4gf83cta7v2","_id":"cmh4jpy0j0003xoc1e4fb4tgq"},{"post_id":"cmh4jpy0i0002xoc18kq6dcrj","tag_id":"cmh4jpy0j0004xoc13j982zo4","_id":"cmh4jpy0l0007xoc14l70fx5d"}],"Tag":[{"name":"AI","_id":"cmgnsr37n0004f4gfdnux759b"},{"name":"机器学习","_id":"cmgnsr37p0008f4gf83cta7v2"},{"name":"-[论文阅读] -[调研]","_id":"cmh4jpy0j0004xoc13j982zo4"}]}}