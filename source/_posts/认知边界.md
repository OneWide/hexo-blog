---
layout: posts
title: 认知边界
date: 2025-11-07 16:13:44
tags: 
 - [论文阅读]
 - [调研]
categories: 
 - [科研]
---

# 认知边界综述调研
> 文章名：Knowledge Boundary of Large Language Models: A Survey

## 为什么进行这项研究
LLM容易生成不真实信息，或者被不真实的背景误导，也可能会对不清楚的查询缺乏精准度。

## 综述做了什么
### 提出对知识边界的定义
提出了形式化的LLM知识边界，从三个维度对知识进行分类：

1. 知识是否为人类所知道的，是否可以以文本QA形式表达（通用知识边界）
2. 是否抽象的嵌入在LLM的参数中（参数知识边界）
3. 是否在LLM上得到经验验证（外显知识边界）

针对上述三个类型的知识边界，文章构建了一个四类知识分类法，对每种知识类型进行分类和定义。

![](../images/认知边界三种方法.png)

定义上，把K比奥是为人类已知的整个抽象知识的集合，把k表示为可以由一组输入输出对${Q_k}=\{(q^i_k,a^i_k) \}$,把$\theta$表示为特定LLM的参数，${\hat{Q}_k} \subseteq Q_k$
* Prompt-Agnostic Known Knowledge (PAK)：无论prompt是什么都可以通过LLM $\theta$  的 ${\hat{Q}_k}$ 中所有表达式进行验证，也就是预测输出的概率大于阈值$\epsilon$
* Prompt-Sensitive Known Knowledge (PSK):在LLM参数范围内，对提示的形式很敏感，虽然在${\hat{Q}_k}$中不一定能验证此类知识，但是在$Q_k$中可以找到适当的表达式来验证
* Model-Specific Unknown Knowledge (MSU)：在特定的LLM参数$\theta$中找不到，因此不能被LLM的$Q_k$中的任何prompt验证，但是该知识是人类已知的，即$Q_k$非空
* Model-Agnostic Unknown Knowledge (MAU)：对人类来说是未知的，无论模型如何都无法验证

### 知识边界的危害
#### 上下文误导的错误反应
尽管LLM拥有所需的知识，但还是会被上下文影响产生不真实的反应，一般有不真实上下文二号无关上下文两种方法
* 通过添加虚假背景信息来影响LLM的判断
* 不相关上下文影响LLM判断

#### 事实幻觉
指的是模型由于特定的未知知识，输出与现实世界存在偏差。
* 缺乏特定领域的知识，这会导致特定领域的查询不准确，由于知识不足而产生幻觉。
* 过时知识。数据集受到时间限制，如果没有更新内部知识的机制，往往会产生幻觉；此外，尽管使用新的数据训练，也可能会出现倾向于使用早年数据的幻觉。
* 对未知知识的过度自信。由于LLM的奖励系统泛化能力有限，过度拟合熟悉的数据并且疏忽少见的主题的话，就会导致自信被放大。LLM还缺乏表明不确定性和承认知识限制的机制，这加剧了过度自信的问题

#### 真实但是错误的回答
LLM在处理与模型无关的知识时可能会产生不正确的响应。
* 针对模糊的测试，LLM无法识别歧义而猜测答案，对模棱两可的查询提供随即性质的答案。
* 对有争议的知识的偏见反应。对有争议的知识涉及主管问题，答案取决于个人观点。

### 如何识别知识边界
分为三类：不确定性估计、置信度校准、内部状态探测。

**不确定性估计：** 量化模型对给定输入的预测的不确定性，不确定性高代表输入的相关知识处在边界之外。不确定性估计主要有下面几种方案：
* 不确定性分解：把LLM的不确定性分解为认知不确定性和任意不确定性。认知不确定性是指model-specific的不确定性,与对参数知识边界的定义有关；任意不确定性指数据层面的不确定性，如有多个有效答案的模糊prompt，这是外显知识边界和参数知识边界的差距。对这些不确定想的量化方案可以分为**数据侧**和**模型侧**两种方法，数据侧包括输入侧清理和扰动、输出侧变化估计；模型侧包括模型参数和构造扰动与模型内部状态扰动。
* 相似性预测：识别一组输出来量化模型输出的不确定性，并保证正确输出包含在该组中的概率。
* 基于token概率的不确定性估计：源于传统的 UE，基于简单令牌概率的 UE 计算平均令牌概率或 LLM 预测的熵作为不确定性。详细设计涉及考虑token级别之外的不同预测粒度。
* 基于语义的不确定性：大致分为基于一致性的方法和语言化的方法。基于一致性的方法将输入的多个采样预测之间的不一致视为不确定性。衡量采样输出语义一致性的方法包括通过较小模型计算的语义距离。

**置信度校准**。校准是指估计的 LLM 置信度与实际预测正确性之间的一致性。低置信度表明预测可能不准确，表明法学硕士可能缺乏某些知识。将现有方法分为基于提示的方法和微调方法。
* 基于提示词的校准：一组方法旨在用提示词激发LLM的自信心，根据预测概率通过抽样作为 LLM 置信度的衡量标准；另一组方法旨在促使LLM直接将置信度表达为预测中的标记。
* 微调方法：包括自我更新 LLM 参数和调整其他模型进行校准。自更新涉及置信度表达的指令调整，以及学习调整输出标记概率；可以训练其他模型来调整 LLM 输出概率以进行校准

**内部状态探测**涉及注意力头、隐藏层激活、神经元和token
### 如何解决这个问题
不是我们关注的重点，略过

# 论文阅读：Cognitive Overload: Jailbreaking Large Language Models with  Overloaded Logical Thinking
做了针对LLM的认知结构和过程的越狱攻击。具体包含三种方案：多语言认知超载、隐晦表达、果因推理。这是一种黑盒攻击（和我们目标的元认知攻击一样），使用AdvBench和MasterKey数据集进行试验。[开源代码链接](https://github.com/luka-groupCognitiveOverload.git.)
## 核心内容
收到心理学研究中的认知负荷理论启发，该理论植于对人类认知结构的理解。理论表明当认知负荷超过有限的工作记忆容量时就会发生认知超载，导致学习和推理的结果受到阻碍。

这篇文章在这项工作中的三种主要攻击方案如下：
![](../images/认知超载攻击.png)
* 多语言认知超载：通过用各种语言（特别是低资源语言）和语言切换场景提出有害问题来检查LLM的安全机制。
* 隐晦表达：在有害的prompt中用含蓄的表达来解释恶意的词语。
* 果因推理：构建了一个虚拟角色，该角色因为某种特定原因被指控但是最终被无罪释放，然后提示LLM列出该角色的潜在恶意行为而不受法律制裁。

认知架构的基础上，认知负荷研究者从学习任务和学习环境的角度开发了多种管理认知负荷的方法，本工作在此得到启发，从两个方向防御：情景防御、防御指示。
* 情景防御：类似新手学生考虑学习任务设计的工作示例，通过提供包含有害Prompt的演示以及作为上下文的适当响应
* 防御指示：对原始系统指令补充特定指令语句，避免认知过载造成的混淆。


# 测试结论（暂时）
LLM应该有文本过滤功能，无关自身的认知安全策略，只是先对Prompt进行文本分析。

这里LLM的认知边界就在于：让我做的是“xx”，“xx”是有害的，我不能做

想要元认知越狱，只能对数据集的表述进行修改，换成另一种表达（类似论文2），让LLM的文本过滤意识不到Prompt本身是一个有害行为
